{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe zum Gradientenverfahren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we gather all imports\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# MinMaxScaler for optional data normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# just for some optional plots e.g. for plotting error functions\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# we import pandas only since it is needed by ppscore\n",
    "import pandas as pd\n",
    "\n",
    "# ppscore for optional exploratory data analysis\n",
    "import ppscore as pps\n",
    "\n",
    "# MinMaxScaler for optional exploratory data analysis\n",
    "from scipy import stats\n",
    "\n",
    "# the following can be used for type hints regarding numpy arrays\n",
    "# however there is an open question if this includes also scalars\n",
    "# or whether to support also scalars would require another type for the type hints\n",
    "# or perhaps an or-combination of ArrayLike with another type \n",
    "#from numpy.typing import ArrayLike\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we load the data\n",
    "\n",
    "# this is the number of data points aka. instances in the given dataset\n",
    "number_of_known_data_points = 14\n",
    "\n",
    "x_known_unsorted = np.asarray([84,1323,282,957,1386,810,396,474,501,660,1260,1005,1110,1290]).astype('float32')\n",
    "assert x_known_unsorted.shape == (number_of_known_data_points,)\n",
    "assert x_known_unsorted.dtype == 'float32'\n",
    "\n",
    "y_known_unsorted = np.asarray([44,97,30,51,95,51,44,41,21,40,90,83,61,92]).astype('float32')\n",
    "assert y_known_unsorted.shape == (number_of_known_data_points,)\n",
    "assert y_known_unsorted.dtype == 'float32'\n",
    "\n",
    "dataset_known_unsorted = np.asarray([ x_known_unsorted, y_known_unsorted ]).transpose()\n",
    "print(f\"dataset_known_unsorted=\\n{dataset_known_unsorted}\")\n",
    "assert dataset_known_unsorted.shape == (number_of_known_data_points,2)\n",
    "assert dataset_known_unsorted.dtype == 'float32'\n",
    "\n",
    "# in this code section we obtain the same matrix however sorted by x\n",
    "# the following call would sort all columns individually which we do not want\n",
    "#dataset_known.sort(axis=0)\n",
    "# instead here we want to sort the rows by the values in the first column\n",
    "dataset_known_intermediate_view = dataset_known_unsorted[:, 0]\n",
    "dataset_known_intermediate_indices_for_sorting = dataset_known_intermediate_view.argsort()\n",
    "print(dataset_known_intermediate_indices_for_sorting)\n",
    "dataset_known = dataset_known_unsorted[dataset_known_intermediate_indices_for_sorting]\n",
    "assert dataset_known.shape == (number_of_known_data_points,2)\n",
    "assert dataset_known.dtype == 'float32'\n",
    "print(dataset_known.shape)\n",
    "print(f\"dataset_known=\\n{dataset_known}\")\n",
    "\n",
    "# we sorted dataset_known by the x values so now we get the x_known sorted as well\n",
    "x_known = dataset_known[:,0]\n",
    "assert x_known.shape == (number_of_known_data_points,)\n",
    "assert x_known.dtype == 'float32'\n",
    "print(f\"x_known=\\n{x_known}\")\n",
    "\n",
    "# we sorted dataset_known by the x values so now we get the x_known sorted as well\n",
    "y_known = dataset_known[:,1]\n",
    "assert y_known.shape == (number_of_known_data_points,)\n",
    "assert y_known.dtype == 'float32'\n",
    "print(f\"y_known=\\n{y_known}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we do an EDA\n",
    "\n",
    "# plain line plot\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x_known,y_known,linestyle='',marker='.',markersize=11.,markerfacecolor='#8080ff',markeredgewidth=.0)\n",
    "plt.title('data_known')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n",
    "\n",
    "# location parameters\n",
    "print(f\"mean={dataset_known.mean(axis=0)}\")\n",
    "print(f\"trimmed_mean={stats.trim_mean(dataset_known.astype('float32'), proportiontocut=0.10, axis=0)}\")\n",
    "print(f\"mode={stats.mode(dataset_known, keepdims=True)}\")\n",
    "\n",
    "# statistical dispersion measures\n",
    "def range_np(a: np.ndarray) -> np.ndarray:\n",
    "    result = a.max(axis=0) - a.min(axis=0)\n",
    "    return result\n",
    "\n",
    "print(f\"range={range_np(dataset_known)}\")\n",
    "print(f\"iqr={stats.iqr(dataset_known, axis=0)}\")\n",
    "\n",
    "print(f\"percentile_10={np.percentile(dataset_known, 10.0, axis=0)}\")\n",
    "print(f\"percentile_25={np.percentile(dataset_known, 25.0, axis=0)}\")\n",
    "print(f\"median={np.percentile(dataset_known, 50.0, axis=0)}\")\n",
    "print(f\"percentile_75={np.percentile(dataset_known, 75.0, axis=0)}\")\n",
    "print(f\"percentile_90={np.percentile(dataset_known, 90.0, axis=0)}\")\n",
    "\n",
    "def mad_np(a: np.ndarray) -> np.ndarray:\n",
    "    result = np.mean(np.absolute(a - np.mean(a, axis=0)), axis=0)\n",
    "    return result\n",
    "\n",
    "print(f\"mad={mad_np(dataset_known)}\")\n",
    "\n",
    "print(f\"std={dataset_known.std(axis=0)}\")\n",
    "print(f\"var={dataset_known.var(axis=0)}\")\n",
    "\n",
    "# association measures\n",
    "print(f\"\\ncorrelation_matrix=\\n{np.corrcoef(dataset_known, rowvar=False).round(decimals=2)}\")\n",
    "\n",
    "dataset_known_pd = pd.DataFrame(dataset_known, columns = ['x','y'])\n",
    "predictive_power_score_matrix_all_pd = pps.matrix(dataset_known_pd, output='df')\n",
    "predictive_power_score_matrix_all_pd.style.background_gradient(cmap='twilight', low=0.0, high=1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we normalize the data\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(dataset_known)\n",
    "\n",
    "# note that we do not need to sort again after scaler.transform(...)\n",
    "# nor do we need to switch back to data type float32\n",
    "# since the function preserves order and data type\n",
    "\n",
    "dataset_known_norm = scaler.transform(dataset_known)\n",
    "assert dataset_known_norm.shape == (number_of_known_data_points,2)\n",
    "assert dataset_known.dtype == 'float32'\n",
    "print(dataset_known_norm.shape)\n",
    "print(f\"dataset_known_norm=\\n{dataset_known}\")\n",
    "\n",
    "x_known_norm = dataset_known_norm[:,0]\n",
    "assert x_known_norm.shape == (number_of_known_data_points,)\n",
    "assert x_known_norm.dtype == 'float32'\n",
    "print(f\"x_known_norm=\\n{x_known_norm}\")\n",
    "\n",
    "y_known_norm = dataset_known_norm[:,1]\n",
    "assert y_known_norm.shape == (number_of_known_data_points,)\n",
    "assert y_known_norm.dtype == 'float32'\n",
    "print(f\"y_known_norm=\\n{y_known_norm}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we run the EDA again this time on the normalized data\n",
    "\n",
    "# plain line plot\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x_known_norm,y_known_norm,linestyle='',marker='.',markersize=11.,markerfacecolor='#8080ff',markeredgewidth=.0)\n",
    "plt.title('data_known_norm')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n",
    "\n",
    "# location parameters\n",
    "print(f\"mean={dataset_known_norm.mean(axis=0)}\")\n",
    "print(f\"trimmed_mean={stats.trim_mean(dataset_known_norm.astype('float32'), proportiontocut=0.10, axis=0)}\")\n",
    "print(f\"mode={stats.mode(dataset_known_norm, keepdims=True)}\")\n",
    "\n",
    "# statistical dispersion measures\n",
    "def range_np(a: np.ndarray) -> np.ndarray:\n",
    "    result = a.max(axis=0) - a.min(axis=0)\n",
    "    return result\n",
    "\n",
    "print(f\"range={range_np(dataset_known_norm)}\")\n",
    "print(f\"iqr={stats.iqr(dataset_known_norm, axis=0)}\")\n",
    "\n",
    "print(f\"percentile_10={np.percentile(dataset_known_norm, 10.0, axis=0)}\")\n",
    "print(f\"percentile_25={np.percentile(dataset_known_norm, 25.0, axis=0)}\")\n",
    "print(f\"median={np.percentile(dataset_known_norm, 50.0, axis=0)}\")\n",
    "print(f\"percentile_75={np.percentile(dataset_known_norm, 75.0, axis=0)}\")\n",
    "print(f\"percentile_90={np.percentile(dataset_known_norm, 90.0, axis=0)}\")\n",
    "\n",
    "def mad_np(a: np.ndarray) -> np.ndarray:\n",
    "    result = np.mean(np.absolute(a - np.mean(a, axis=0)), axis=0)\n",
    "    return result\n",
    "\n",
    "print(f\"mad={mad_np(dataset_known_norm)}\")\n",
    "\n",
    "print(f\"std={dataset_known_norm.std(axis=0)}\")\n",
    "print(f\"var={dataset_known_norm.var(axis=0)}\")\n",
    "\n",
    "# association measures\n",
    "print(f\"\\ncorrelation_matrix=\\n{np.corrcoef(dataset_known_norm, rowvar=False).round(decimals=2)}\")\n",
    "\n",
    "dataset_known_norm_pd = pd.DataFrame(dataset_known_norm, columns = ['x','y'])\n",
    "predictive_power_score_matrix_all_pd = pps.matrix(dataset_known_norm_pd, output='df')\n",
    "predictive_power_score_matrix_all_pd.style.background_gradient(cmap='twilight', low=0.0, high=1.0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define our model function\n",
    "\n",
    "# design aspects\n",
    "# only one feature\n",
    "# purely linear without any non-linear terms\n",
    "# degree of the polynomial is 1\n",
    "# thus also only one weight per feature\n",
    "# b as an additive weight representing bias\n",
    "\n",
    "# first in pure python for single x values\n",
    "# note that the expressions behind the colons and behind the -> are just type hints\n",
    "# in this case all hinting at float\n",
    "# they might be disregarded as python in general does not check the types of values on function call\n",
    "def f_pure_python(w : float, x: float, b: float) -> float:\n",
    "    y = w * x + b\n",
    "    return y\n",
    "\n",
    "# now with numpy for any numpy array of any number of x values\n",
    "\n",
    "# w stands for the weight or weights\n",
    "# x stands for one or more x values\n",
    "# b stands for the bias\n",
    "\n",
    "def f(w: np.ndarray, x: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    # note that these fancy pre-condition checks are out-commented\n",
    "    # so as to also support cases in which w or b or x is provided as a plain vanilla python float\n",
    "    # pre-condition checks\n",
    "    #assert w.shape == (), \"w must be a numpy scalar\"\n",
    "    #assert np.isscalar(w), \"w must be a numpy scalar\"\n",
    "    #assert w.dtype == 'float32', \"w must be of dtype float32\"\n",
    "    #assert b.shape == (), \"b must be a numpy scalar\"\n",
    "    #assert np.isscalar(b), \"b must be a numpy scalar\"\n",
    "    #assert b.dtype == 'float32', \"b must be of dtype float32\"\n",
    "    #assert x.dtype == 'float32', \"xs must be of dtype float32\"\n",
    "    y = w * x + b\n",
    "    # note that these fancy post-condition checks are out-commented\n",
    "    # so as to also support cases in which w or b or x are provided as a plain vanilla floats\n",
    "    # interestingly this basically makes the function definition the same as in the pure form above\n",
    "    # post-condition checks\n",
    "    #assert y.dtype == 'float32', \"y must be of dtype float32\"\n",
    "    #assert y.shape == x.shape, \"y must be of same shape as xs\"\n",
    "    return y\n",
    "\n",
    "# again this time as a numpy ufunc for better efficiency in numpy contexts\n",
    "\n",
    "# in this case we chose to include _np_ufunc in the variable name referencing the function\n",
    "# just to distinguish it from the above variants of the same function\n",
    "\n",
    "f_np_ufunc = np.frompyfunc(f, 3, 1)\n",
    "\n",
    "assert type(f_pure_python) != np.ufunc\n",
    "assert type(f) != np.ufunc\n",
    "assert type(f_np_ufunc) == np.ufunc\n",
    "\n",
    "# note that functions wrapped via frompyfunc always return numpy arrays with dtype object\n",
    "\n",
    "# also note that the speed advantage provided by making a ufunc via frompyfunc\n",
    "# will be irrelevant if numba is used\n",
    "# see https://numba.readthedocs.io/en/stable/user/5minguide.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we just try out the model function\n",
    "\n",
    "print(f\"x_known=\\n{x_known}\")\n",
    "print(f\"y_known=\\n{y_known}\")\n",
    "\n",
    "# our more carefully chosen examples for w and b as plain python floats\n",
    "w1_scalar = .058\n",
    "assert np.isscalar(w1_scalar)\n",
    "b_scalar = 16.2\n",
    "assert np.isscalar(b_scalar)\n",
    "\n",
    "# try out the numpy version with our known x values\n",
    "y_line_example_2 = f(w1_scalar, x_known, b_scalar)\n",
    "assert y_line_example_2.shape == y_known.shape\n",
    "assert y_line_example_2.dtype == 'float32'\n",
    "assert not np.isscalar(y_line_example_2)\n",
    "print(f\"y_line_example_2=\\n{np.round(y_line_example_2, 3)}\")\n",
    "\n",
    "# our more carefully chosen examples for w and b as numpy arrays\n",
    "w1_np = np.asarray([w1_scalar], dtype='float32')\n",
    "assert w1_np.shape == (1,)\n",
    "assert not np.isscalar(w1_np)\n",
    "b_np = np.asarray([b_scalar], dtype='float32')\n",
    "assert b_np.shape == (1,)\n",
    "assert not np.isscalar(b_np)\n",
    "\n",
    "# check if it also works if we provide the one weight and the bias as numpy arrays\n",
    "y_line_example = f(w1_np, x_known, b_np)\n",
    "assert y_line_example.shape == y_known.shape\n",
    "assert y_line_example.dtype == 'float32'\n",
    "assert not np.isscalar(y_line_example)\n",
    "print(f\"y_line_example=\\n{np.round(y_line_example, 3)}\")\n",
    "\n",
    "# check if it also works if we provide the one weight and the bias as numpy arrays\n",
    "y_line_example_from_np_ufunc = f_np_ufunc(w1_np, x_known, b_np).astype('float32')\n",
    "assert y_line_example_from_np_ufunc.shape == y_known.shape\n",
    "print(f\"y_line_example_from_np_ufunc=\\n{y_line_example_from_np_ufunc}\")\n",
    "print(f\"y_line_example_from_np_ufunc.shape=\\n{y_line_example_from_np_ufunc.shape}\")\n",
    "print(f\"y_line_example_from_np_ufunc.dtype=\\n{y_line_example_from_np_ufunc.dtype}\")\n",
    "\n",
    "# the values in the last two result arrays should equal\n",
    "assert (y_line_example == y_line_example_2).all()\n",
    "#assert (y_line_example == y_line_example_from_np_ufunc).all()\n",
    "\n",
    "# just a plain line plot of the model function regarding a try out with normalized data\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x_known,y_known,linestyle='',marker='.',markersize=11.,markerfacecolor='#8080ff',markeredgewidth=.0)\n",
    "plt.plot(x_known,y_line_example,color='#dd99ff')\n",
    "plt.title('modelled_example')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we just try out the model function again this time with the normalized data\n",
    "\n",
    "print(f\"x_known_norm=\\n{x_known_norm}\")\n",
    "print(f\"y_known_norm=\\n{y_known_norm}\")\n",
    "\n",
    "# our more carefully chosen examples for w and b as plain python floats\n",
    "w1_norm_scalar = 1.\n",
    "assert np.isscalar(w1_norm_scalar)\n",
    "b_norm_scalar = .0001\n",
    "assert np.isscalar(b_norm_scalar)\n",
    "\n",
    "# try out the numpy version with our known x values\n",
    "y_line_norm_example_2 = f(w1_norm_scalar, x_known_norm, b_norm_scalar)\n",
    "assert y_line_norm_example_2.shape == y_known_norm.shape\n",
    "assert y_line_norm_example_2.dtype == 'float32'\n",
    "assert not np.isscalar(y_line_norm_example_2)\n",
    "print(f\"y_line_norm_example_2=\\n{np.round(y_line_norm_example_2, 3)}\")\n",
    "\n",
    "# our more carefully chosen examples for w and b as numpy arrays\n",
    "w1_norm_np = np.asarray([w1_norm_scalar], dtype='float32')\n",
    "assert w1_norm_np.shape == (1,)\n",
    "assert not np.isscalar(w1_norm_np)\n",
    "b_norm_np = np.asarray([b_norm_scalar], dtype='float32')\n",
    "assert b_norm_np.shape == (1,)\n",
    "assert not np.isscalar(b_norm_np)\n",
    "\n",
    "# check if it also works if we provide the one weight and the bias as numpy arrays\n",
    "y_line_norm_example = f(w1_norm_np, x_known_norm, b_norm_np)\n",
    "assert y_line_norm_example.shape == y_known_norm.shape\n",
    "assert y_line_norm_example.dtype == 'float32'\n",
    "assert not np.isscalar(y_line_norm_example)\n",
    "print(f\"y_line_norm_example=\\n{np.round(y_line_norm_example, 3)}\")\n",
    "\n",
    "# the values in the last two result arrays should equal\n",
    "assert (y_line_norm_example == y_line_norm_example_2).all()\n",
    "\n",
    "# just a plain line plot of the model function regarding a try out with normalized data\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x_known_norm,y_line_norm_example,color='#dd99ff')\n",
    "plt.plot(x_known_norm,y_known_norm,linestyle='',marker='.',markersize=11.,markerfacecolor='#8080ff',markeredgewidth=.0)\n",
    "plt.title('modelled_norm_example')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define the error functions\n",
    "\n",
    "# for each error function we actually write down an efficient usual version and an explicit version\n",
    "# the main difference is that\n",
    "# the efficient versions expect the computed y-predictions as parameters\n",
    "# the explicit versions expect the model function its parameters and call it\n",
    "\n",
    "# also the efficient versions happen to be one-liners\n",
    "# which does not say anything about performance\n",
    "\n",
    "# y are the actual known values\n",
    "# x are the corresponding x values\n",
    "# f is the model function\n",
    "# w is the weight used in the model function\n",
    "# b is the bias used in the model function\n",
    "# y_pred are the predictions computed using the model function\n",
    "\n",
    "# sse formula is sum((y-f(x))^2)\n",
    "\n",
    "def sse(y, y_hat):    \n",
    "    return np.square(y - y_hat).sum()\n",
    "\n",
    "def sse_verbosely_implemented(y, y_hat):\n",
    "    y_diff = y - y_hat\n",
    "    squares = y_diff ** 2\n",
    "    sum_of_squares = squares.sum()\n",
    "    return sum_of_squares\n",
    "\n",
    "def sse_explicit(y, x, f, w, b):\n",
    "    # what does the model say?\n",
    "    y_pred = f(w, x, b)\n",
    "    # what is the difference to the actual result\n",
    "    y_diff = y - y_pred\n",
    "    # squared\n",
    "    squares = np.square(y_diff)\n",
    "    # summed up\n",
    "    sum_of_squares = squares.sum()\n",
    "    return sum_of_squares\n",
    "\n",
    "# mse formula is sum((y-f(x))^2) / n\n",
    "\n",
    "def mse(y, y_pred):\n",
    "    return sse(y, y_pred) / len(y)\n",
    "\n",
    "def mse_explicit(y, x, f, w, b):\n",
    "    sum_of_squares = sse_explicit(y, x, f, w, b)\n",
    "    # divided by the number of data points\n",
    "    n = len(x)\n",
    "    mse_value = sum_of_squares / n\n",
    "    return mse_value\n",
    "\n",
    "# rmse formula is sqrt(sum((y-f(x))^2) / n)\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mse(y, y_pred))\n",
    "\n",
    "def rmse_explicit(y, x, f, w, b):\n",
    "    mse_value = mse_explicit(y, x, f, w, b)\n",
    "    # square root thereof\n",
    "    rmse_value = np.sqrt(mse_value)\n",
    "    return rmse_value\n",
    "\n",
    "# mae formula is sum(|y-f(x)|) / n\n",
    "\n",
    "def mae(y, y_pred):\n",
    "    return np.abs(y - y_pred).sum() / len(y)\n",
    "\n",
    "def mae_explicit(y, x, f, w, b):\n",
    "    # what does the model say?\n",
    "    y_m = f(w, x, b)\n",
    "    # what is the difference to the actual result\n",
    "    y_diff = y - y_m\n",
    "    # the absolute of that\n",
    "    y_diff_abs = np.abs(y_diff)\n",
    "    # summed up\n",
    "    y_diff_abs_sum = y_diff_abs.sum()    \n",
    "    # divided by the number of data points\n",
    "    n = len(x)\n",
    "    mae_value = y_diff_abs_sum / n\n",
    "    return mae_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we try out the efficiently implemented error functions\n",
    "\n",
    "def error_functions_try_out(y, f, x, w1_scalar, w1_np, b_scalar, b_np):\n",
    "    # check if it works if we provide w and b as pure python floats\n",
    "\n",
    "    y_pred_using_scalars = f(w1_scalar, x, b_scalar)\n",
    "    sse_example_using_scalars = sse(y, y_pred_using_scalars)\n",
    "    assert np.isscalar(sse_example_using_scalars)\n",
    "    print(f\"sse_example_using_scalars={sse_example_using_scalars}\")\n",
    "\n",
    "    mse_example_using_scalars = mse(y, y_pred_using_scalars)\n",
    "    assert np.isscalar(mse_example_using_scalars)\n",
    "    print(f\"mse_example_using_scalars={mse_example_using_scalars}\")\n",
    "\n",
    "    rmse_example_using_scalars = rmse(y, y_pred_using_scalars)\n",
    "    assert np.isscalar(rmse_example_using_scalars)\n",
    "    print(f\"rmse_example_using_scalars={rmse_example_using_scalars}\")\n",
    "\n",
    "    mae_example_using_scalars = mae(y, y_pred_using_scalars)\n",
    "    assert np.isscalar(mae_example_using_scalars)\n",
    "    print(f\"mae_example_using_scalars={mae_example_using_scalars}\")\n",
    "\n",
    "    # check if it also works if we provide w and b as numpy objects\n",
    "    y_pred_np = f(w1_np, x, b_np)\n",
    "    sse_example = sse(y, y_pred_np)\n",
    "    assert np.isscalar(sse_example)\n",
    "    print(f\"sse_example={sse_example}\")\n",
    "\n",
    "    mse_example = mse(y, y_pred_np)\n",
    "    assert np.isscalar(mse_example)\n",
    "    print(f\"mse_example={mse_example}\")\n",
    "\n",
    "    rmse_example = rmse(y, y_pred_np)\n",
    "    assert np.isscalar(rmse_example)\n",
    "    print(f\"rmse_example={rmse_example}\")\n",
    "\n",
    "    mae_example = mae(y, y_pred_np)\n",
    "    assert np.isscalar(mae_example)\n",
    "    print(f\"mae_example={mae_example}\")\n",
    "\n",
    "    assert sse_example_using_scalars == sse_example\n",
    "    assert mse_example_using_scalars == mse_example\n",
    "    assert rmse_example_using_scalars == rmse_example\n",
    "    assert mae_example_using_scalars == mae_example\n",
    "\n",
    "    return (sse_example, mse_example, rmse_example, mae_example)\n",
    "\n",
    "# try with the original data\n",
    "sse_example, mse_example, rmse_example, mae_example = error_functions_try_out(y_known, f, x_known, w1_scalar, w1_np, b_scalar, b_np)\n",
    "\n",
    "# try with the normalized data\n",
    "sse_example_norm, mse_example_norm, rmse_example_norm, mae_example_norm = error_functions_try_out(y_known_norm, f, x_known_norm, w1_norm_scalar, w1_norm_np, b_norm_scalar, b_norm_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we try out the explicitly implemented error functions\n",
    "\n",
    "def explicit_error_functions_try_out(y, f, x, w1_scalar, w1_np, b_scalar, b_np):\n",
    "    # check if it works if we provide w and b as pure python floats\n",
    "    sse_explicit_example_using_scalars = sse_explicit(y, x, f, w1_scalar, b_scalar)\n",
    "    assert np.isscalar(sse_explicit_example_using_scalars)\n",
    "    print(f\"sse_explicit_example_using_scalars={sse_explicit_example_using_scalars}\")\n",
    "\n",
    "    mse_explicit_example_using_scalars = mse_explicit(y, x, f, w1_scalar, b_scalar)\n",
    "    assert np.isscalar(mse_explicit_example_using_scalars)\n",
    "    print(f\"mse_explicit_example_using_scalars={mse_explicit_example_using_scalars}\")\n",
    "\n",
    "    rmse_explicit_example_using_scalars = rmse_explicit(y, x, f, w1_scalar, b_scalar)\n",
    "    assert np.isscalar(rmse_explicit_example_using_scalars)\n",
    "    print(f\"rmse_explicit_example_using_scalars={rmse_explicit_example_using_scalars}\")\n",
    "\n",
    "    mae_explicit_example_using_scalars = mae_explicit(y, x, f, w1_scalar, b_scalar)\n",
    "    assert np.isscalar(mae_explicit_example_using_scalars)\n",
    "    print(f\"mae_explicit_example_using_scalars={mae_explicit_example_using_scalars}\")\n",
    "\n",
    "    # check if it also works if we provide w and b as numpy objects\n",
    "    sse_explicit_example = sse_explicit(y, x, f, w1_np, b_np)\n",
    "    assert np.isscalar(sse_explicit_example)\n",
    "    print(f\"sse_explicit_example={sse_explicit_example}\")\n",
    "\n",
    "    mse_explicit_example = mse_explicit(y, x, f, w1_np, b_np)\n",
    "    assert np.isscalar(mse_explicit_example)\n",
    "    print(f\"mse_explicit_example={mse_explicit_example}\")\n",
    "\n",
    "    rmse_explicit_example = rmse_explicit(y, x, f, w1_np, b_np)\n",
    "    assert np.isscalar(rmse_explicit_example)\n",
    "    print(f\"rmse_explicit_example={rmse_explicit_example}\")\n",
    "\n",
    "    mae_explicit_example = mae_explicit(y, x, f, w1_np, b_np)\n",
    "    assert np.isscalar(mae_explicit_example)\n",
    "    print(f\"mae_explicit_example={mae_explicit_example}\")\n",
    "\n",
    "    assert sse_explicit_example_using_scalars == sse_explicit_example\n",
    "    assert mse_explicit_example_using_scalars == mse_explicit_example\n",
    "    assert rmse_explicit_example_using_scalars == rmse_explicit_example\n",
    "    assert mae_explicit_example_using_scalars == mae_explicit_example\n",
    "\n",
    "    return (sse_explicit_example, mse_explicit_example, rmse_explicit_example, mae_explicit_example)\n",
    "\n",
    "# try with the original data\n",
    "sse_explicit_example, mse_explicit_example, rmse_explicit_example, mae_explicit_example = explicit_error_functions_try_out(y_known, f, x_known, w1_scalar, w1_np, b_scalar, b_np)\n",
    "\n",
    "# try with the normalized data\n",
    "sse_explicit_example_norm, mse_explicit_example_norm, rmse_explicit_example_norm, mae_explicit_example_norm = explicit_error_functions_try_out(y_known_norm, f, x_known_norm, w1_norm_scalar, w1_norm_np, b_norm_scalar, b_norm_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code we check example results of our error functions\n",
    "\n",
    "# error values computed by explicitly implemented error functions should equal the usual ones\n",
    "assert sse_explicit_example == sse_example\n",
    "assert mse_explicit_example == mse_example\n",
    "assert rmse_explicit_example == rmse_example\n",
    "assert mae_explicit_example == mae_example\n",
    "\n",
    "# also for the normalized data\n",
    "assert sse_explicit_example_norm == sse_example_norm\n",
    "assert mse_explicit_example_norm == mse_example_norm\n",
    "assert rmse_explicit_example_norm == rmse_example_norm\n",
    "assert mae_explicit_example_norm == mae_example_norm\n",
    "\n",
    "# however error values for normalized data should differ from error values for original data\n",
    "assert sse_explicit_example != sse_example_norm\n",
    "assert mse_explicit_example != mse_example_norm\n",
    "assert rmse_explicit_example != rmse_example_norm\n",
    "assert mae_explicit_example != mae_example_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we attempt to plot the error functions with respect to original data\n",
    "\n",
    "# first let us create a sorted numpy array of arbitrary values for w1 i.e. the first weight\n",
    "w1s_for_plot_np = np.arange(.0, 0.1001, .0001).astype('float32')\n",
    "\n",
    "# we chose a value for b which we keep constant over all w1s\n",
    "b_for_plot_scalar = b_scalar\n",
    "\n",
    "# note that in functional programming we would use currying instead\n",
    "def f_with_fixed_xs_and_b(some_w1):\n",
    "   return f(some_w1, x_known, b_for_plot_scalar)\n",
    "\n",
    "# we still need to vectorize the function to be able to apply it to each element of a numpy array\n",
    "f_with_fixed_xs_and_b_vectorized = np.vectorize(f_with_fixed_xs_and_b,otypes=[np.ndarray])\n",
    "\n",
    "# we compute the predictions for each w1\n",
    "y_pred_for_plot_np = f_with_fixed_xs_and_b_vectorized(w1s_for_plot_np)\n",
    "\n",
    "# compute sse values\n",
    "def sse_with_fixed_y(some_y_pred):\n",
    "   return sse(y_known, some_y_pred)\n",
    "\n",
    "sse_with_fixed_y_vectorized = np.vectorize(sse_with_fixed_y ,otypes=[np.ndarray])\n",
    "sse_axis_values_np = np.apply_along_axis(sse_with_fixed_y_vectorized, axis=0, arr=y_pred_for_plot_np)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(w1s_for_plot_np, sse_axis_values_np,color='#ffb380')\n",
    "plt.title('sse')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n",
    "\n",
    "# compute mse values\n",
    "def mse_with_fixed_y(some_y_pred):\n",
    "   return mse(y_known, some_y_pred)\n",
    "\n",
    "mse_with_fixed_y_vectorized = np.vectorize(mse_with_fixed_y ,otypes=[np.ndarray])\n",
    "mse_axis_values_np = np.apply_along_axis(mse_with_fixed_y_vectorized, axis=0, arr=y_pred_for_plot_np)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(w1s_for_plot_np, mse_axis_values_np,color='#ffb380')\n",
    "plt.title('mse')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n",
    "\n",
    "# compute rmse values\n",
    "def rmse_with_fixed_y(some_y_pred):\n",
    "   return rmse(y_known, some_y_pred)\n",
    "\n",
    "rmse_with_fixed_y_vectorized = np.vectorize(rmse_with_fixed_y ,otypes=[np.ndarray])\n",
    "rmse_axis_values_np = np.apply_along_axis(rmse_with_fixed_y_vectorized, axis=0, arr=y_pred_for_plot_np)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(w1s_for_plot_np, rmse_axis_values_np,color='#ffb380')\n",
    "plt.title('rmse')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n",
    "\n",
    "# compute mae values\n",
    "def mae_with_fixed_y(some_y_pred):\n",
    "   return mae(y_known, some_y_pred)\n",
    "\n",
    "mae_with_fixed_y_vectorized = np.vectorize(mae_with_fixed_y ,otypes=[np.ndarray])\n",
    "mae_axis_values_np = np.apply_along_axis(mae_with_fixed_y_vectorized, axis=0, arr=y_pred_for_plot_np)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(w1s_for_plot_np, mae_axis_values_np,color='#ffb380')\n",
    "plt.title('mae')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we attempt to plot the error functions with respect to normalized data\n",
    "\n",
    "# first let us create a sorted numpy array of arbitrary values for w1 i.e. the first weight\n",
    "w1s_for_plot_norm_np = np.arange(.699, 1.101, .00005).astype('float32')\n",
    "\n",
    "# we chose a value for b which we keep constant over all w1s\n",
    "b_for_plot_norm_scalar = b_norm_scalar\n",
    "\n",
    "# note that in functional programming we would use currying instead\n",
    "def f_with_fixed_xs_and_b_norm(some_w1_norm):\n",
    "   return f(some_w1_norm, x_known_norm, b_for_plot_norm_scalar)\n",
    "\n",
    "# we still need to vectorize the function to be able to appky it to each element of a numpy array\n",
    "f_with_fixed_xs_and_b_norm_vectorized = np.vectorize(f_with_fixed_xs_and_b_norm,otypes=[np.ndarray])\n",
    "\n",
    "# we compute the predictions for each w1\n",
    "y_pred_norm_for_plot_np = f_with_fixed_xs_and_b_norm_vectorized(w1s_for_plot_norm_np)\n",
    "\n",
    "# compute sse values\n",
    "def sse_with_fixed_y_norm(some_y_pred_norm):\n",
    "   return sse(y_known_norm, some_y_pred_norm)\n",
    "\n",
    "sse_with_fixed_y_norm_vectorized = np.vectorize(sse_with_fixed_y_norm ,otypes=[np.ndarray])\n",
    "sse_axis_values_norm_np = np.apply_along_axis(sse_with_fixed_y_norm_vectorized, axis=0, arr=y_pred_norm_for_plot_np)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(w1s_for_plot_norm_np, sse_axis_values_norm_np, color='#ffb380')\n",
    "plt.title('sse norm')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n",
    "\n",
    "# compute mse values\n",
    "def mse_with_fixed_y_norm(some_y_pred_norm):\n",
    "   return mse(y_known_norm, some_y_pred_norm)\n",
    "\n",
    "mse_with_fixed_y_norm_vectorized = np.vectorize(mse_with_fixed_y_norm ,otypes=[np.ndarray])\n",
    "mse_axis_values_norm_np = np.apply_along_axis(mse_with_fixed_y_norm_vectorized, axis=0, arr=y_pred_norm_for_plot_np)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(w1s_for_plot_norm_np, mse_axis_values_norm_np, color='#ffb380')\n",
    "plt.title('mse norm')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n",
    "\n",
    "# compute rmse values\n",
    "def rmse_with_fixed_y_norm(some_y_pred_norm):\n",
    "   return rmse(y_known_norm, some_y_pred_norm)\n",
    "\n",
    "rmse_with_fixed_y_norm_vectorized = np.vectorize(rmse_with_fixed_y_norm ,otypes=[np.ndarray])\n",
    "rmse_axis_values_norm_np = np.apply_along_axis(rmse_with_fixed_y_norm_vectorized, axis=0, arr=y_pred_norm_for_plot_np)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(w1s_for_plot_norm_np, rmse_axis_values_norm_np, color='#ffb380')\n",
    "plt.title('rmse norm')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()\n",
    "\n",
    "# compute mae values\n",
    "def mae_with_fixed_norm_y(some_y_pred_norm):\n",
    "   return mae(y_known_norm, some_y_pred_norm)\n",
    "\n",
    "mae_with_fixed_y_norm_vectorized = np.vectorize(mae_with_fixed_norm_y ,otypes=[np.ndarray])\n",
    "mae_axis_values_norm_np = np.apply_along_axis(mae_with_fixed_y_norm_vectorized, axis=0, arr=y_pred_norm_for_plot_np)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(w1s_for_plot_norm_np, mae_axis_values_norm_np, color='#ffb380')\n",
    "plt.title('mae norm')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "# in this code block we define the gradient functions with respect to the error functions\n",
    "\n",
    "# derivatives as seen at\n",
    "# https://spia.uga.edu/faculty_pages/mlynch/teaching/ols/OLSDerivation.pdf\n",
    "# https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc\n",
    "\n",
    "# note that many people rearrange the terms slightly to get rid of the minus in front\n",
    "\n",
    "# we use the dot product\n",
    "# which is just the sum of the products of the corresponding elements of two vectors\n",
    "\n",
    "# partial derivative of sse with respect to w1\n",
    "\n",
    "def sse_gradient_4_linear_regression_wrt_w_digethic(y_here,y_hat_here,x_here):\n",
    "    y_diff_here = y_here - y_hat_here\n",
    "    print(f\"y_diff_here={y_diff_here}\")\n",
    "    return -2 * np.dot(x_here, y_diff_here)\n",
    "\n",
    "def sse_gradient_4_linear_regression_wrt_w(y,w,x,b):\n",
    "    result = -2 * np.dot(x,(y - ((w * x) + b)))\n",
    "    return result\n",
    "\n",
    "# partial derivative of sse with respect to b\n",
    "def sse_gradient_4_linear_regression_wrt_b(y,w,x,b):\n",
    "    result = -2 * np.sum(y - ((w * x) - b))\n",
    "    return result\n",
    "\n",
    "# partial derivative of mse with respect to w1\n",
    "def mse_gradient_4_linear_regression_wrt_w(y,w,x,b):\n",
    "    n = len(x)\n",
    "    result = -2 * np.reciprocal(n) * np.sum(np.dot(x,(y - ((w*x)+b))))\n",
    "    return result\n",
    "\n",
    "# partial derivative of mse with respect to b\n",
    "def mse_gradient_4_linear_regression_wrt_b(y,w,x,b):\n",
    "    n = len(x)\n",
    "    result = -2 * np.reciprocal(n) * np.sum((y - ((w*x)+b)))\n",
    "    return result\n",
    "\n",
    "#def rmse_gradient():\n",
    "#    return ()\n",
    "\n",
    "#def mae_gradient():\n",
    "#    return ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1_guess=0.0531776\n",
      "b_np=[16.2]\n",
      "x_known=[  84.  282.  396.  474.  501.  660.  810.  957. 1005. 1110. 1260. 1290.\n",
      " 1323. 1386.]\n",
      "y_known=[44. 30. 44. 41. 21. 40. 51. 51. 83. 61. 90. 92. 97. 95.]\n",
      "y_hat_at=[20.66692  31.196083 37.25833  41.40618  42.84198  51.297215 59.273857\n",
      " 67.09096  69.643486 75.227135 83.20378  84.7991   86.55397  89.90416 ]\n",
      "sse_at=2137.489501953125\n",
      "y_diff_here=[ 23.33308     -1.1960831    6.7416687   -0.40618134 -21.84198\n",
      " -11.2972145   -8.273857   -16.090958    13.356514   -14.227135\n",
      "   6.796219     7.200897    10.44603      5.0958405 ]\n",
      "gradient_at=67.0146484375\n",
      "gradient_at_2=67.0146484375\n"
     ]
    }
   ],
   "source": [
    "w1_guess = 0.0531776\n",
    "\n",
    "print(f\"w1_guess={w1_guess}\")\n",
    "print(f\"b_np={b_np}\")\n",
    "print(f\"x_known={x_known}\")\n",
    "print(f\"y_known={y_known}\")\n",
    "\n",
    "y_hat_at = f(w1_guess, x_known, b_np)\n",
    "print(f\"y_hat_at={y_hat_at}\")\n",
    "\n",
    "sse_zero_check = sse(y_known, y_known)\n",
    "assert sse_zero_check == 0.0\n",
    "\n",
    "sse_at = sse(y_known, y_hat_at)\n",
    "print(f\"sse_at={sse_at}\")\n",
    "\n",
    "gradient_at = sse_gradient_4_linear_regression_wrt_w_digethic(y_known, y_hat_at, x_known) \n",
    "print(f\"gradient_at={gradient_at}\")\n",
    "\n",
    "gradient_at_2 = sse_gradient_4_linear_regression_wrt_w(y_known, w1_guess, x_known, b_np)\n",
    "print(f\"gradient_at_2={gradient_at_2}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Weights randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block our weight values are initialized in a very explicit verbose fashion\n",
    "\n",
    "# this is the number of weights in our model function\n",
    "# so it is also going to be the number of values in our vector of weights w\n",
    "number_of_weights = 2\n",
    "\n",
    "# in the following the variables prefixed with w_pre_ are just intermediate variables\n",
    "# the variable w_init is the vector of our weights after initialization\n",
    "\n",
    "# in this example we draw random values based on a normal distribution\n",
    "# we actually draw two more values than number_of_weights\n",
    "# since during the following min-max-normalization\n",
    "# the smallest value, the min, always becomes .0 (or at least very close to it owed to floats)\n",
    "# and the largest value, the max, always becomes 1. (or at least very close to it owed to floats)\n",
    "w_pre_number_of_values = number_of_weights + 2\n",
    "w_pre_random_values_float64 = np.random.normal(loc=.5, scale=.2, size=w_pre_number_of_values)\n",
    "# note that the random function yields float64 however we want float32 all the way\n",
    "w_pre_random_values = w_pre_random_values_float64.astype('float32')\n",
    "assert w_pre_random_values.shape == (w_pre_number_of_values,)\n",
    "assert w_pre_random_values.dtype == 'float32'\n",
    "print(f\"w_pre={w_pre_random_values}\")\n",
    "\n",
    "# the values in w_pre_initialized can be below 0 and above 1 so we min-max-normalize them\n",
    "# in this case just out of curiosity through explicit code based on numpy primitives\n",
    "# instead of using functionality from some library such as the MinMaxScaler from scikit-learn\n",
    "# formula for min-max-normalization is value_min_max_norm = value - min / max - min\n",
    "\n",
    "# the min of the drawn random values and note that scalars have shape ()\n",
    "w_pre_min = w_pre_random_values.min()\n",
    "assert w_pre_min.shape == ()\n",
    "assert w_pre_min.dtype == 'float32'\n",
    "print(f\"w_pre_min={w_pre_min}\")\n",
    "\n",
    "# the max of the drawn random values\n",
    "w_pre_max = w_pre_random_values.max()\n",
    "assert w_pre_max.shape == ()\n",
    "assert w_pre_max.dtype == 'float32'\n",
    "print(f\"w_pre_max={w_pre_max}\")\n",
    "\n",
    "# the range\n",
    "w_pre_range = w_pre_max - w_pre_min\n",
    "assert w_pre_range.shape == ()\n",
    "assert w_pre_range.dtype == 'float32'\n",
    "print(f\"w_pre_range={w_pre_range}\")\n",
    "\n",
    "# 1 over the range and note how the division yields float64 however we want float32 all the way\n",
    "w_pre_range_reciprocal = (1 / w_pre_range).astype('float32')\n",
    "assert w_pre_range_reciprocal.shape == ()\n",
    "assert w_pre_range_reciprocal.dtype == 'float32'\n",
    "print(f\"w_pre_range_reciprocal={w_pre_range_reciprocal}\")\n",
    "\n",
    "# an array of the same shape as w_pre_random_values and where all values equal w_pre_min\n",
    "w_pre_min_repeated = np.full_like(w_pre_random_values, w_pre_min)\n",
    "assert w_pre_min_repeated.shape == (w_pre_number_of_values,)\n",
    "assert w_pre_min_repeated.dtype == 'float32'\n",
    "print(f\"w_pre_min_repeated={w_pre_min_repeated}\")\n",
    "\n",
    "# an array with all the random values after subtracting w_pre_min from them\n",
    "w_pre_minus_min = np.subtract(w_pre_random_values, w_pre_min_repeated)\n",
    "assert w_pre_minus_min.shape == (w_pre_number_of_values,)\n",
    "assert w_pre_minus_min.dtype == 'float32'\n",
    "print(f\"w_pre_minus_min={w_pre_minus_min}\")\n",
    "\n",
    "# the array with the min-max-normalized random values\n",
    "w_pre_min_max_norm = w_pre_minus_min * w_pre_range_reciprocal\n",
    "assert w_pre_min_max_norm.shape == (w_pre_number_of_values,)\n",
    "assert w_pre_min_max_norm.dtype == 'float32'\n",
    "print(f\"w_pre_min_max_norm={w_pre_min_max_norm}\")\n",
    "\n",
    "# we now mask the values that are equal or close to the minimum .0 or the maximum .1\n",
    "w_pre_masked = np.ma.masked_outside(w_pre_min_max_norm, .001, .999, copy=True)\n",
    "assert w_pre_masked.shape == (w_pre_number_of_values,)\n",
    "assert w_pre_masked.dtype == 'float32'\n",
    "print(f\"w_pre_masked={w_pre_masked}\")\n",
    "\n",
    "# to finally drop them using the function compressed() of the masking facility to obtain our weights\n",
    "w_init_float = w_pre_masked.compressed()\n",
    "print(f\"\\nw_init={w_init_float}\")\n",
    "assert w_init_float.shape == (number_of_weights,)\n",
    "assert w_init_float.dtype == 'float32'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad23019639f4f0cec1969a685ba2a43be98d322ffbcf7e3409a6239a86c6a8ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Introduction\n",
    "\n",
    "Aufgabe:\n",
    "\n",
    "* Trainieren Sie ein mehrschichtiges Neuronales Netz mit PyTorch.\n",
    "* Zeigen Sie grafisch auf, dass Ihr Modell konvergiert.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we gather all imports\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Data Splitting and Normalization is still often done using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# numpy is just used in the beginning\n",
    "import numpy as np\n",
    "\n",
    "# for some plots and the scatter matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps as cm\n",
    "\n",
    "# pandas for reading the CSV and for use with the library ppscore\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# ppscore for exploratory data analysis\n",
    "import ppscore as pps\n",
    "\n",
    "# more statistics for exploratory data analysis\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('This Computation is running on {}.'.format(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we read the CSV file and perform some basic preprocessing\n",
    "\n",
    "# read the CSV file\n",
    "# using separator character semicolon\n",
    "dataset_known_pd = pd.read_csv(\"../../data/iris_binary.csv\", sep=',', skipinitialspace=True)\n",
    "\n",
    "# make column names pythonic\n",
    "# so that they can be used in code where applicable\n",
    "dataset_known_pd.columns = dataset_known_pd.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "# on a side note we choose to sort the data frame by the first column \n",
    "dataset_known_pd.sort_values(by='sepal_length', ascending=True, axis=0, inplace=True)\n",
    "\n",
    "dataset_known_np = dataset_known_pd.to_numpy()\n",
    "\n",
    "# number of target variables\n",
    "n_targets = 1\n",
    "print(f\"n_targets={n_targets}\")\n",
    "\n",
    "# number of target classes\n",
    "n_target_classes = 2\n",
    "print(f\"n_target_classes={n_target_classes}\")\n",
    "\n",
    "# number of instances\n",
    "n_samples = dataset_known_np.shape[0]\n",
    "print(f\"n_samples={n_samples}\")\n",
    "\n",
    "# number of features\n",
    "n_features = dataset_known_np.shape[1] - n_targets\n",
    "print(f\"n_features={n_features}\")\n",
    "\n",
    "assert dataset_known_pd.shape == (n_samples, n_features + n_targets)\n",
    "assert dataset_known_np.shape == (n_samples, n_features + n_targets)\n",
    "\n",
    "X_pd = dataset_known_pd.copy().drop('class', axis=1)\n",
    "X_np = X_pd.to_numpy()\n",
    "assert X_np.shape == (n_samples,n_features)\n",
    "\n",
    "y_pd = dataset_known_pd['class'].copy()\n",
    "y_np = y_pd.to_numpy()\n",
    "assert y_np.shape == (n_samples,)\n",
    "\n",
    "X = torch.from_numpy(X_np)\n",
    "y = torch.from_numpy(y_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_known_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we split the dataset randomly into train and test data\n",
    "\n",
    "# we first attempted to do this purely in PyTorch which is still a bit difficult\n",
    "#dataset_known = torch.from_numpy(dataset_known_np)\n",
    "#dataset_known_subsets = torch.utils.data.random_split(dataset_known, [int(n_samples * 0.7), int(n_samples * 0.3)])\n",
    "#dataset_known_train_subset = dataset_known_subsets[0]\n",
    "#dataset_known_test_subset = dataset_known_subsets[1]\n",
    "#assert len(dataset_known_train_subset) == 105\n",
    "#assert len(dataset_known_test_subset) == 45\n",
    "\n",
    "# however many people still use pandas and sklearn which we follow for now\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=77)\n",
    "\n",
    "assert X_train.ndim == 2\n",
    "assert X_test.ndim == 2\n",
    "assert X_train.shape[0] + X_test.shape[0] == n_samples\n",
    "assert X_train.shape[1] == n_features\n",
    "assert X_test.shape[1] == n_features\n",
    "assert X_train.dtype == torch.float64\n",
    "assert X_test.dtype == torch.float64\n",
    "\n",
    "assert (n_targets == 1 and y_train.ndim == 1) or (n_targets > 1 and y_train.ndim == 2)\n",
    "assert (n_targets == 1 and y_test.ndim == 1) or (n_targets > 1 and y_test.ndim == 2)\n",
    "assert y_train.shape[0] + y_test.shape[0] == n_samples\n",
    "assert y_train.dtype == torch.int64\n",
    "assert y_test.dtype == torch.int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define a model\n",
    "\n",
    "class ClassificationANNModel(nn.Module):\n",
    "    def __init__(self,in_ndim,out_ndim):\n",
    "        super(ClassificationANNModel,self).__init__()\n",
    "        self.input_layer    = nn.Linear(in_ndim,64,dtype=torch.float64)\n",
    "        self.hidden_layer_1  = nn.Linear(64,32,dtype=torch.float64)\n",
    "        self.output_layer   = nn.Linear(32,out_ndim,dtype=torch.float64)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out_1 = self.relu(self.input_layer(X))\n",
    "        out_2 = self.relu(self.hidden_layer_1(out_1))\n",
    "        out_3 = self.output_layer(out_2)\n",
    "        return out_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we create a model\n",
    "\n",
    "model = ClassificationANNModel(n_features,n_target_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we chose loss function and an optimizer\n",
    "\n",
    "lr = 0.001\n",
    "loss = nn.BCELoss()\n",
    "#loss = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define a helper function to get from the raw model output to classes\n",
    "\n",
    "def logits_to_classes(y_logits):\n",
    "    assert y_logits.dtype == torch.float64\n",
    "    print(y_logits[0])\n",
    "\n",
    "    # apply the logistic function to the model output to obtain values between 0 and 1\n",
    "    y_sigmoid = torch.sigmoid(y_logits)\n",
    "    assert y_sigmoid.dtype == torch.float64\n",
    "    print(y_sigmoid[0])\n",
    "\n",
    "    # round the values to obtain the class values\n",
    "    y_classes = torch.round(y_sigmoid)\n",
    "    assert y_classes.dtype == torch.float64\n",
    "    print(y_classes[0])\n",
    "\n",
    "    return  y_classes   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define the training process\n",
    "\n",
    "# ls_train is a tensor to store the loss values w.r.t. training data after each epoch\n",
    "# ls_test is a tensor to store the loss values w.r.t test data after each epoch\n",
    "# debug can be set to True to print information during every 10th epoch\n",
    "def train_model(n_epochs,model,optimizer,loss,X_train,y_train,ls_train,X_test,y_test,ls_test,debug):\n",
    "    assert X_train.dtype == torch.float64\n",
    "    assert X_test.dtype == torch.float64\n",
    "\n",
    "    y_train_float64 = y_train\n",
    "    if y_test.dtype != torch.float64:\n",
    "        y_train_float64 = y_train.double()\n",
    "    assert y_train_float64.dtype == torch.float64\n",
    "\n",
    "    y_test_float64 = y_test\n",
    "    if y_test.dtype != torch.float64:\n",
    "        y_test_float64 = y_test.double()\n",
    "    assert y_test_float64.dtype == torch.float64\n",
    "\n",
    "    # loop through the epochs one after the other\n",
    "    for epoch in range(n_epochs):\n",
    "        # predict with respect to the whole training dataset\n",
    "        y_pred_classes_train = logits_to_classes(model(X_train).squeeze())\n",
    "        \n",
    "        # compute training loss\n",
    "        l_train = loss(y_pred_classes_train, y_train)\n",
    "        \n",
    "        # clear gradients from a previous epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward propagation to compute gradients\n",
    "        l_train.backward()\n",
    "\n",
    "        # now update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            # perform a test to see how good the model is now\n",
    "            y_pred_classes_test = logits_to_classes(model(X_test).squeeze())\n",
    "\n",
    "            # compute test loss\n",
    "            l_test = loss(y_pred_classes_test, y_test)\n",
    "\n",
    "        ls_train[epoch] = l_train.item()\n",
    "        ls_test[epoch] = l_test.item()\n",
    "\n",
    "        if (epoch) % 10 == 0:\n",
    "            if debug:\n",
    "                print(f\"epoch                  {epoch}\")\n",
    "                print(f\"ls_train[epoch]        {ls_train[epoch]}\")\n",
    "                print(f\"ls_test[epoch]         {ls_test[epoch]}\")\n",
    "                print(f\"----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we perform a training of the model\n",
    "\n",
    "# note that we already put the model to the device before and now we put the data to the device\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "n_epochs = 1000\n",
    "ls_train = torch.from_numpy(np.zeros(n_epochs, dtype='float64'))\n",
    "ls_test  = torch.from_numpy(np.zeros(n_epochs, dtype='float64'))\n",
    "ls_train = ls_train.to(device)\n",
    "ls_test = ls_train.to(device)\n",
    "debug = True\n",
    "\n",
    "train_model(n_epochs, model, optimizer, loss, X_train, y_train, ls_train, X_test, y_test, ls_test, debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we visualize the training loss and the test loss\n",
    "\n",
    "plt.figure(figsize=(20,20),dpi=200)\n",
    "plt.plot(ls_train, label='train loss', color='#ffb380')\n",
    "plt.plot(ls_test, label='test loss', color='#eeb070')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad23019639f4f0cec1969a685ba2a43be98d322ffbcf7e3409a6239a86c6a8ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

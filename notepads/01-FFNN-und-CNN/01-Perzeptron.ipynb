{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perzeptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we design, implement, train and try out a perceptron as a binary classifier for the given iris dataset.\n",
    "\n",
    "The dataset is in this case a simplified version of the original dataset from the UCI.\n",
    "While the original dataset has three target classes, this simplified version has only two.\n",
    "The number of instances is the same. See https://archive.ics.uci.edu/ml/datasets/iris for the original.\n",
    "\n",
    "* The four features `xi` are `x1` named `sepal_length`, `x2` named `sepal_width`, `x3` named `petal length`, `x4` named `petal width`.\n",
    "* Together the features form the vector `x`.\n",
    "* Note that the unit of measurement of width and length is in this case unknown, however also not relevant to the task at hand.\n",
    "\n",
    "The dataset has one binary target.\n",
    "\n",
    "* The target is `y` with its column name `class` is categorical and binary.\n",
    "* Note that where we actually use the categorical data type of the library pandas for it, we call the column `class_cat`.\n",
    "\n",
    "Our perceptron is going to have one weight per feature and the additive weiht b for the bias.\n",
    "\n",
    "* We call the weights `wi` simply ```w1, w2, w3, w4```, each of which corresponds to the feature `xi` that has the same index `i`.\n",
    "* Together these four weights form the vector `w`.\n",
    "* The additive weight `b` represents the bias and is in this case stored outside of `w`.\n",
    "\n",
    "Our perceptron yields binary output.\n",
    "\n",
    "* The prediction obtained at the output of the perceptron we call `y_hat`.\n",
    "* The difference between a known result `y` and a predicted result `y_hat` we call `y_diff`.\n",
    "* Note that `y` and `y_hat` can also be vectors in case multiple results and corresponding predictions are referred to.\n",
    "\n",
    "Our perceptron has a learning rate built in.\n",
    "\n",
    "* The lerning rate alpha we call `lr`.\n",
    "\n",
    "The perceptron has a model function.\n",
    "\n",
    "* The function `perceptron_model_func` is the model function of the perceptron.\n",
    "* It is a Heaviside function, i.e. a step function.\n",
    "* It has two steps with a singularity just before 0.\n",
    "* It accepts the vector `w` with all the non-additive weights.\n",
    "* It accepts the vector `x` in order to make multiple predictions at once.\n",
    "* It accepts `b` as a scalar.\n",
    "\n",
    "The perceptron has another function to train it.\n",
    "\n",
    "* The function `perceptron_train_func` trains the perceptron.\n",
    "* It contains the part that updates the weights `w` and `b`.\n",
    "* It accepts known data `x` and `y` which should be vectors, i.e. multiple known instances.\n",
    "* It also requires initial weight values which can just be initialized randomly.\n",
    "* Internally it uses the function `perceptron_model_func` to compute `y_hat` in each iteration. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we gather all imports\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# MinMaxScaler for data normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# for some plots and the scatter matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps as cm\n",
    "\n",
    "# pandas for reading the CSV and for use with the library ppscore\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# ppscore for exploratory data analysis\n",
    "import ppscore as pps\n",
    "\n",
    "# more statistics for exploratory data analysis\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we read the CSV file and perform some basic preprocessing\n",
    "\n",
    "# read the CSV file\n",
    "# using separator character semicolon\n",
    "df_all_pd = pd.read_csv(\"../../data/iris_binary.csv\", sep=',', skipinitialspace=True)\n",
    "\n",
    "# make column names pythonic\n",
    "# so that they can be used in code where applicable\n",
    "df_all_pd.columns = df_all_pd.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "# on a side note we choose to sort the data frame by the first column \n",
    "df_all_pd.sort_values(by='sepal_length', ascending=True, axis=0, inplace=True)\n",
    "\n",
    "dataset_known = df_all_pd.to_numpy()\n",
    "\n",
    "x_known_pd = df_all_pd.copy().drop('class', axis=1)\n",
    "x_known = x_known_pd.to_numpy()\n",
    "\n",
    "y_known_pd = df_all_pd['class'].copy()\n",
    "y_known = y_known_pd.to_numpy()\n",
    "\n",
    "n = dataset_known.shape[0]\n",
    "assert dataset_known.shape == (n,5)\n",
    "assert x_known.shape == (n,4)\n",
    "assert y_known.shape == (n,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we do an EDA\n",
    "\n",
    "# number of instances\n",
    "print(f\"n={n}\")\n",
    "\n",
    "# location parameters\n",
    "print(f\"mean={dataset_known.mean(axis=0)}\")\n",
    "print(f\"trimmed_mean={stats.trim_mean(dataset_known.astype('float32'), proportiontocut=0.10, axis=0)}\")\n",
    "print(f\"mode={stats.mode(dataset_known, keepdims=True)}\")\n",
    "\n",
    "# statistical dispersion measures\n",
    "def range_np(a: np.ndarray) -> np.ndarray:\n",
    "    result = a.max(axis=0) - a.min(axis=0)\n",
    "    return result\n",
    "\n",
    "print(f\"range={range_np(dataset_known)}\")\n",
    "print(f\"iqr={stats.iqr(dataset_known, axis=0)}\")\n",
    "\n",
    "print(f\"percentile_10={np.percentile(dataset_known, 10.0, axis=0)}\")\n",
    "print(f\"percentile_25={np.percentile(dataset_known, 25.0, axis=0)}\")\n",
    "print(f\"median={np.percentile(dataset_known, 50.0, axis=0)}\")\n",
    "print(f\"percentile_75={np.percentile(dataset_known, 75.0, axis=0)}\")\n",
    "print(f\"percentile_90={np.percentile(dataset_known, 90.0, axis=0)}\")\n",
    "\n",
    "def mad_np(a: np.ndarray) -> np.ndarray:\n",
    "    result = np.mean(np.absolute(a - np.mean(a, axis=0)), axis=0)\n",
    "    return result\n",
    "\n",
    "print(f\"mad={mad_np(dataset_known)}\")\n",
    "\n",
    "print(f\"std={dataset_known.std(axis=0)}\")\n",
    "print(f\"var={dataset_known.var(axis=0)}\")\n",
    "\n",
    "# association measures\n",
    "print(f\"\\ncorrelation_matrix=\\n{np.corrcoef(dataset_known, rowvar=False).round(decimals=2)}\")\n",
    "\n",
    "# we have a look at a scatter matrix\n",
    "pd.plotting.scatter_matrix(df_all_pd, \n",
    "                           c=df_all_pd['class'], \n",
    "                           figsize=(17, 17),\n",
    "                           cmap = cm['cool'],\n",
    "                           diagonal = 'kde')\n",
    "\n",
    "# for the computation of predictive power scores we use pandas categorical data type for class_cat\n",
    "class_categories_pd = CategoricalDtype(categories=[0, 1], ordered=True)\n",
    "df_all_with_class_cat_pd = df_all_pd.copy()\n",
    "df_all_with_class_cat_pd['class_cat'] = df_all_pd['class'].astype(class_categories_pd)\n",
    "df_all_with_class_cat_pd.drop('class', axis=1)\n",
    "\n",
    "#predictive_power_score_matrix_all_pd = pps.matrix(df_pd_all, output='df')\n",
    "predictive_power_scores_pd = pps.predictors(df_all_with_class_cat_pd, y='class_cat', output='df')\n",
    "predictive_power_scores_pd.style.background_gradient(cmap='twilight', low=0.0, high=1.0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we initialize the vector w and the bias b\n",
    "\n",
    "#w_init = np.asarray([0.5, 1.0, -0.5, -1.0])\n",
    "w_init = np.asarray([-0.0, 1.0, 1.0, 1.0])\n",
    "b_init = 0.0005\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define the model function\n",
    "\n",
    "def perceptron_model_func(w, x, b):\n",
    "    # thanks to the use of np.matmul(x, w)\n",
    "    # w can be the vector w with all weights\n",
    "    # x can be a matrix of many instances with len(w) columns and n rows\n",
    "    # b can be a scalar or a numpy array of shape (1,)\n",
    "    assert w.shape == (4,)\n",
    "    assert x.shape == (150, w.shape[0])\n",
    "    assert np.isscalar(b) or b.shape == (1,)\n",
    "\n",
    "    y_pure = np.matmul(x, w).round(4) + b\n",
    "\n",
    "    assert not np.isscalar(y_pure)\n",
    "    assert y_pure.dtype == 'float64'\n",
    "    assert y_pure.shape == (x.shape[0],)\n",
    "    \n",
    "    y_hat = np.heaviside(y_pure, 1)\n",
    "\n",
    "    assert not np.isscalar(y_hat)\n",
    "    assert y_hat.dtype == 'float64'\n",
    "    assert y_hat.shape == (x.shape[0],)\n",
    "    return y_hat\n",
    "\n",
    "# try out the model function once with a twist and some assertions\n",
    "\n",
    "# for this example we use a b that will be recognizable in the y_pure values\n",
    "b_init_example_1 = 0.0005\n",
    "# and we use a w1 equals zero to effectively deactivate that feature\n",
    "w_init_example_1 = np.asarray([-0.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "y_pure_example_1 = np.matmul(x_known, w_init_example_1).round(4) + b_init\n",
    "assert not np.isscalar(y_pure_example_1)\n",
    "assert y_pure_example_1.dtype == 'float64'\n",
    "assert y_pure_example_1.shape == (150,)\n",
    "\n",
    "y_hat_example_1 = perceptron_model_func(w_init_example_1, x_known, b_init_example_1)\n",
    "assert not np.isscalar(y_hat_example_1)\n",
    "assert y_hat_example_1.dtype == 'float64'\n",
    "assert y_hat_example_1.shape == (150,)\n",
    "\n",
    "for j in range(n):\n",
    "    #print(f\"x{j}={x_known[j]}\")\n",
    "    #print(f\"y_pure_example_1{j}={y_pure_example_1[j]}\")\n",
    "    # we use the especially chosen b_init_example_1 and w_init_example_1\n",
    "    # to be able to do the following assertion\n",
    "    y_pure_example_1_expected = x_known[j,1:].sum().round(2) + 0.0005\n",
    "    #print(f\"y_pure_example_1_expected={y_pure_example_1_expected}\")\n",
    "    assert y_pure_example_1_expected == y_pure_example_1[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define the more simple model function needed during training\n",
    "\n",
    "# the difference to the main model function above is that the simple version\n",
    "# only works for one instance x_j at a time\n",
    "\n",
    "def perceptron_model_func_for_train(w_j, x_j, b_j):\n",
    "    assert w_j.shape == (4,)\n",
    "    assert x_j.shape == w_j.shape\n",
    "    assert np.isscalar(b_j) or b_j.shape == (1,)\n",
    "    y_hat_pure_j = np.dot(w_j, x_j) + b_j\n",
    "    y_hat_j = np.heaviside(y_hat_pure_j, 1)\n",
    "    return y_hat_j\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Update Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we experiment with the implementation for learning with respect to vector w\n",
    "\n",
    "# note that the function in this code block is experimental\n",
    "# since it computes all learning steps in one operation based on numpy broadcasting\n",
    "# which however does not meet the requirements of the perceptron learning approach\n",
    "# thus for the actual function to update the weights see next code block\n",
    "\n",
    "# formula is wi_new = wi + lr * (y - y_hat) * xij\n",
    "# index i is the index of the feature\n",
    "# index j is the index of the known instance\n",
    "\n",
    "def perceptron_train_func_experimental(w, lr, y, y_hat, x):\n",
    "    # w can be the vector w with all weights\n",
    "    # x can be a matrix of many instances with len(w) columns and n rows\n",
    "    # b can be a scalar or a numpy array of shape (1,)\n",
    "    \n",
    "    # number of features i.e. number of weights i.e. number of columns\n",
    "    m_local = w.shape[0]\n",
    "\n",
    "    # number of given instances i.e. rows\n",
    "    n_local = y.shape[0]\n",
    "\n",
    "    # check the shapes of the vectors w, y, y_hat\n",
    "    assert w.shape == (m_local,)\n",
    "    assert y.shape == (n_local,)\n",
    "    assert y_hat.shape == (n_local,)\n",
    "    # check the shape of the matrix x\n",
    "    assert x.shape == (n_local, m_local)\n",
    "\n",
    "    y_diff = y - y_hat\n",
    "    assert y_diff.shape == (n_local,)\n",
    "\n",
    "    # making use of the numpy broadcasting rules\n",
    "    # we add an empty dimension to y_diff to prepare for the following multiplication\n",
    "    y_diff_expanded = np.expand_dims(y_diff, axis=1)\n",
    "    assert y_diff_expanded.shape == (n_local, 1)\n",
    "\n",
    "    y_diff_x = y_diff_expanded * x\n",
    "    assert y_diff_x.shape == (n_local, m_local)\n",
    "\n",
    "    lr_y_diff_x = lr * y_diff_x\n",
    "\n",
    "    w_new = w + lr_y_diff_x\n",
    "    \n",
    "    # this printing was just for debugging purposes\n",
    "    #print(f\"w            {w}\")\n",
    "    #for j in range(n_local):\n",
    "    #    print(f\"x            {x[j]}\")\n",
    "    #    print(f\"y            {y[j]}\")\n",
    "    #    print(f\"y_hat        {y_hat[j]}\")\n",
    "    #    print(f\"y_diff       {y_diff_expanded[j]}\")\n",
    "    #    print(f\"y_diff_x     {y_diff_x[j]}\")\n",
    "    #    print(f\"lr_y_diff_x  {lr_y_diff_x[j]}\")\n",
    "    #    print(f\"w_new        {w_new[j]}\")\n",
    "\n",
    "    assert w_new.shape == (n_local, m_local)\n",
    "    return w_new\n",
    "\n",
    "# try out the function\n",
    "w_new_experimental = perceptron_train_func_experimental(w_init_example_1, 0.01, y_known, y_hat_example_1, x_known)\n",
    "#print(w_new_experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we implement the function to update the weights\n",
    "\n",
    "def perceptron_train_func(w, lr, y, x, b, debug):\n",
    "    # w can be the vector w with all weights\n",
    "    # x can be a matrix of many instances with len(w) columns and n rows\n",
    "    # b can be a scalar or a numpy array of shape (1,)\n",
    "    \n",
    "    # number of features i.e. number of weights i.e. number of columns\n",
    "    m_local = w.shape[0]\n",
    "\n",
    "    # number of given instances i.e. rows\n",
    "    n_local = y.shape[0]\n",
    "\n",
    "    # check the shapes of the vectors w, y, y_hat\n",
    "    assert w.shape == (m_local,)\n",
    "    assert y.shape == (n_local,)    \n",
    "    # check the shape of the matrix x\n",
    "    assert x.shape == (n_local, m_local)\n",
    "\n",
    "    w_j = w\n",
    "    b_j = b\n",
    "    for j in range(n_local):\n",
    "        # for each instance\n",
    "        x_j = x[j]\n",
    "        assert x_j.shape == (m_local,)\n",
    "\n",
    "        y_j = y[j]\n",
    "        assert np.isscalar(y_j)\n",
    "\n",
    "        y_hat_j = perceptron_model_func_for_train(w_j, x_j, b)\n",
    "        assert np.isscalar(y_hat_j)\n",
    "\n",
    "        y_diff_j = y_j - y_hat_j\n",
    "        assert np.isscalar(y_diff_j)\n",
    "\n",
    "        w_change_j = lr * (y_diff_j * x_j)\n",
    "        w_new_j = w_j + w_change_j\n",
    "\n",
    "        b_change_j = lr * (y_diff_j)\n",
    "        b_new_j = b_j + b_change_j\n",
    "\n",
    "        if debug:\n",
    "            print(f\"w_j            {w_j}\")\n",
    "            print(f\"x_j            {x_j}\")\n",
    "            print(f\"y_j            {y_j}\")\n",
    "            print(f\"y_hat_j        {y_hat_j}\")\n",
    "            print(f\"y_diff_j       {y_diff_j}\")\n",
    "            print(f\"change_j       {w_change_j}\")\n",
    "            print(f\"w_new_j        {w_new_j}\")\n",
    "            print(f\"b_change_j     {b_change_j}\")\n",
    "            print(f\"b_new_j        {b_new_j}\")\n",
    "            print(f\"----------------------------------------------------------------\")\n",
    "        w_j = w_new_j\n",
    "        b_j = b_new_j\n",
    "\n",
    "    return (w_j, b_j)\n",
    "\n",
    "# try out the function\n",
    "debug = False\n",
    "w_new_example_1, b_new_example_1 = perceptron_train_func(w_init_example_1, 0.01, y_known, x_known, b_init_example_1, debug)\n",
    "print(w_new_example_1)\n",
    "print(b_new_example_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we first train the perceptron and then use it\n",
    "\n",
    "# train\n",
    "w_trained, b_trained = perceptron_train_func(w_init, 0.01, y_known, x_known, b_init, False)\n",
    "\n",
    "print(f\"w_trained={w_trained}\")\n",
    "print(f\"b_trained={b_trained}\")\n",
    "\n",
    "# apply model function based on matmul\n",
    "y_hat_example_2 = perceptron_model_func(w_trained, x_known, b_trained)\n",
    "\n",
    "# apply naive model function\n",
    "y_hat_example_2_simple = np.zeros(n)\n",
    "for j in range(n):\n",
    "    y_hat_example_2_simple[j] = perceptron_model_func_for_train(w_trained, x_known[j], b_trained)\n",
    "\n",
    "# check the results\n",
    "print(y_known)\n",
    "y_hat_example_2_int = y_hat_example_2.astype('int32')\n",
    "y_hat_example_2_simple_int = y_hat_example_2_simple.astype('int')\n",
    "\n",
    "# mask where values are not equal which should not be the case\n",
    "y_hat_example_2_int_masked = np.ma.masked_where((y_known != y_hat_example_2_int.copy()).all(), y_hat_example_2_int)\n",
    "print(y_hat_example_2_int_masked)\n",
    "assert y_hat_example_2_int_masked.compressed().size == n\n",
    "# mask where values are equal which should everywhere be the case\n",
    "y_hat_example_2_int_masked_negative = np.ma.masked_where((y_known == y_hat_example_2_int.copy()).all(), y_hat_example_2_int)\n",
    "print(y_hat_example_2_int_masked_negative)\n",
    "print(y_hat_example_2_int_masked_negative.compressed())\n",
    "assert y_hat_example_2_int_masked_negative.compressed().size == 0\n",
    "\n",
    "# mask where values are not equal which should not be the case\n",
    "y_hat_example_2_simple_int_masked = np.ma.masked_where((y_known != y_hat_example_2_simple_int.copy()).all(), y_hat_example_2_simple_int)\n",
    "print(y_hat_example_2_simple_int_masked)\n",
    "assert y_hat_example_2_simple_int_masked.compressed().size == n\n",
    "# mask where values are equal which should everywhere be the case\n",
    "y_hat_example_2_simple_int_masked_negative = np.ma.masked_where((y_known == y_hat_example_2_simple_int.copy()).all(), y_hat_example_2_simple_int)\n",
    "print(y_hat_example_2_simple_int_masked_negative)\n",
    "assert y_hat_example_2_simple_int_masked_negative.compressed().size == 0\n",
    "\n",
    "assert (y_known == y_hat_example_2_int).all()\n",
    "assert (y_known == y_hat_example_2_simple_int).all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad23019639f4f0cec1969a685ba2a43be98d322ffbcf7e3409a6239a86c6a8ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we gather all imports\n",
    "\n",
    "# to read CSV and for use with the library ppscore\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# for use with sklearn and for EDA\n",
    "import numpy as np\n",
    "\n",
    "# Data Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Normalization and Standardization\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Plots\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps as cm\n",
    "\n",
    "# EDA\n",
    "import ppscore as pps\n",
    "from scipy import stats\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Decision Tree Classifier\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Tensors and Artificial Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Computation is running on cpu.\n"
     ]
    }
   ],
   "source": [
    "# in this code block we determine the device to use depending on GPU availability\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('This Computation is running on {}.'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define a function to read the dataset and perform some basic preprocessing\n",
    "\n",
    "def load_dataset(path: str, sep: str, target_column_name: str, columns_to_drop: list, column_name_mapping: dict, target_class_mapping: dict):\n",
    "\n",
    "    target_column_is_index = False \n",
    "    target_column_index = -1\n",
    "    if(target_column_name.isnumeric()):\n",
    "        target_column_is_index = True\n",
    "        target_column_index = int(target_column_name)\n",
    "\n",
    "    # read the CSV file\n",
    "    # using separator character semicolon\n",
    "    X_original_y_pd = pd.read_csv(path, sep=sep, skipinitialspace=True)\n",
    "\n",
    "    # make column names pythonic\n",
    "    # so that they can be used in code where applicable\n",
    "    X_original_y_pd.columns = X_original_y_pd.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "    for column_to_drop in columns_to_drop:\n",
    "        X_original_y_pd.drop(column_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    X_original_y_pd.rename(column_name_mapping, axis=1, inplace=True)\n",
    "\n",
    "    X_original_y_pd[target_column_name] = X_original_y_pd[target_column_name].map(target_class_mapping)\n",
    "\n",
    "    # on a side note we choose to sort the data frame by the target column \n",
    "    #X_original_y_pd.sort_values(by='species', ascending=True, axis=0, inplace=True)\n",
    "\n",
    "    X_y_np = X_original_y_pd.to_numpy()\n",
    "\n",
    "    # number of instances often referred to as just n\n",
    "    n_samples = X_y_np.shape[0]\n",
    "    print(f\"n_samples={n_samples}\")\n",
    "\n",
    "    # number of target variables\n",
    "    n_targets = 1\n",
    "    print(f\"n_targets={n_targets}\")\n",
    "\n",
    "    # number of target classes\n",
    "    #n_target_classes = 3\n",
    "    #print(f\"n_target_classes={n_target_classes}\")\n",
    "\n",
    "    # number of features\n",
    "    n_features = X_y_np.shape[1] - n_targets\n",
    "    print(f\"n_features={n_features}\")\n",
    "\n",
    "    assert X_y_np.shape == (n_samples, n_features + n_targets)\n",
    "    assert X_y_np.shape == (n_samples, n_features + n_targets)\n",
    "\n",
    "    X_original_pd = X_original_y_pd.copy().drop(target_column_name, axis=1)\n",
    "    X_original_np = X_original_pd.to_numpy()\n",
    "    assert X_original_np.shape == (n_samples,n_features)\n",
    "\n",
    "    y_pd = X_original_y_pd[target_column_name].copy()\n",
    "    y_np = y_pd.to_numpy()\n",
    "    assert y_np.shape == (n_samples,)\n",
    "\n",
    "    X_original = torch.from_numpy(X_original_np)\n",
    "    y = torch.from_numpy(y_np)\n",
    "\n",
    "    # we need the target data to be of data type long for the loss function to work\n",
    "    if y.dtype != torch.int64:\n",
    "        y = y.long()\n",
    "    assert X_original.dtype == torch.float64\n",
    "    assert y.dtype == torch.int64\n",
    "\n",
    "    # also create a tensor that contains the 4 features and the target\n",
    "    y_unsqueezed = y.unsqueeze(1)\n",
    "    X_original_y = torch.cat((X_original, y_unsqueezed), 1)\n",
    "    assert X_original_y.shape == (n_samples, n_features + n_targets)\n",
    "    assert X_original_y.dtype == torch.float64\n",
    "\n",
    "    return (X_original_pd, y_pd, X_original_np, y_np, X_original, y, X_original_y_pd, X_original_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define a function to normalize a dataset\n",
    "\n",
    "def standardize_dataset(X_original_np):\n",
    "    scaler = StandardScaler()\n",
    "    X_np = scaler.fit_transform(X_original_np)\n",
    "    X = torch.from_numpy(X_np)\n",
    "\n",
    "    return (X_np, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define a function to split the dataset into for training, validation, test\n",
    "\n",
    "def split_dataset(X, y, n_targets):\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    # we first attempted to do this purely in PyTorch which is still a bit difficult\n",
    "    #dataset_known = torch.from_numpy(dataset_known_np)\n",
    "    #dataset_known_subsets = torch.utils.data.random_split(dataset_known, [int(n_samples * 0.7), int(n_samples * 0.3)])\n",
    "    #dataset_known_train_subset = dataset_known_subsets[0]\n",
    "    #dataset_known_test_subset = dataset_known_subsets[1]\n",
    "    #assert len(dataset_known_train_subset) == 105\n",
    "    #assert len(dataset_known_test_subset) == 45\n",
    "\n",
    "    # however many people still use pandas and sklearn which we follow for now\n",
    "    X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=0.20, random_state=77)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.20, random_state=77)\n",
    "\n",
    "    del X_tmp\n",
    "    del y_tmp\n",
    "\n",
    "    assert X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == n_samples\n",
    "    assert y_train.shape[0] + y_val.shape[0] + y_test.shape[0] == n_samples\n",
    "\n",
    "    assert X_train.ndim == 2\n",
    "    assert X_train.shape[1] == n_features\n",
    "    assert X_train.dtype == torch.float64\n",
    "\n",
    "    assert X_val.ndim == 2\n",
    "    assert X_val.shape[1] == n_features\n",
    "    assert X_val.dtype == torch.float64\n",
    "\n",
    "    assert X_test.ndim == 2\n",
    "    assert X_test.shape[1] == n_features\n",
    "    assert X_test.dtype == torch.float64\n",
    "\n",
    "    assert (n_targets == 1 and y_train.ndim == 1) or (n_targets > 1 and y_train.ndim == 2)\n",
    "    assert y_train.dtype == torch.int64\n",
    "\n",
    "    assert (n_targets == 1 and y_val.ndim == 1) or (n_targets > 1 and y_val.ndim == 2)\n",
    "    assert y_val.dtype == torch.int64\n",
    "\n",
    "    assert (n_targets == 1 and y_test.ndim == 1) or (n_targets > 1 and y_test.ndim == 2)\n",
    "    assert y_test.dtype == torch.int64\n",
    "\n",
    "    X_train_np = X_train.numpy()\n",
    "    y_train_np = y_train.numpy()\n",
    "    X_val_np = X_val.numpy()\n",
    "    y_val_np = y_val.numpy()\n",
    "    X_test_np = X_test.numpy()\n",
    "    y_test_np = y_test.numpy()\n",
    "\n",
    "    return (X_train, y_train, X_train_np, y_train_np, X_val, X_val_np, y_val, y_val_np, X_test, X_test_np, y_test, y_test_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define a function to perform an EDA\n",
    "\n",
    "def explore_dataset(X_original_y_pd, X_y_np, n_target_classes, target_column_name):\n",
    "\n",
    "    target_column_cat_name = f\"{target_column_name}_cat\"\n",
    "    n_target_classes_range = range(n_target_classes)\n",
    "    target_classes_int = [*n_target_classes_range]\n",
    "\n",
    "    # number of instances\n",
    "    print(f\"n={X_y_np.shape[0]}\")\n",
    "\n",
    "    # location parameters\n",
    "    print(f\"mean={X_y_np.mean(axis=0)}\")\n",
    "    print(f\"trimmed_mean={stats.trim_mean(X_y_np.astype('float32'), proportiontocut=0.10, axis=0)}\")\n",
    "    print(f\"mode={stats.mode(X_y_np, keepdims=True)}\")\n",
    "\n",
    "    # statistical dispersion measures\n",
    "    def range_np(a: np.ndarray) -> np.ndarray:\n",
    "        result = a.max(axis=0) - a.min(axis=0)\n",
    "        return result\n",
    "\n",
    "    print(f\"range={range_np(X_y_np)}\")\n",
    "    print(f\"iqr={stats.iqr(X_y_np, axis=0)}\")\n",
    "\n",
    "    print(f\"percentile_10={np.percentile(X_y_np, 10.0, axis=0)}\")\n",
    "    print(f\"percentile_25={np.percentile(X_y_np, 25.0, axis=0)}\")\n",
    "    print(f\"median={np.percentile(X_y_np, 50.0, axis=0)}\")\n",
    "    print(f\"percentile_75={np.percentile(X_y_np, 75.0, axis=0)}\")\n",
    "    print(f\"percentile_90={np.percentile(X_y_np, 90.0, axis=0)}\")\n",
    "\n",
    "    def mad_np(a: np.ndarray) -> np.ndarray:\n",
    "        result = np.mean(np.absolute(a - np.mean(a, axis=0)), axis=0)\n",
    "        return result\n",
    "\n",
    "    print(f\"mad={mad_np(X_y_np)}\")\n",
    "\n",
    "    print(f\"std={X_y_np.std(axis=0)}\")\n",
    "    print(f\"var={X_y_np.var(axis=0)}\")\n",
    "\n",
    "    # association measures\n",
    "    print(f\"\\ncorrelation_matrix=\\n{np.corrcoef(X_y_np, rowvar=False).round(decimals=2)}\")\n",
    "\n",
    "    # we have a look at a scatter matrix\n",
    "    pd.plotting.scatter_matrix(X_original_y_pd, \n",
    "                            c=X_original_y_pd[target_column_name], \n",
    "                            figsize=(17, 17),\n",
    "                            cmap = cm['cool'],\n",
    "                            diagonal = 'kde')\n",
    "\n",
    "    # for the computation of predictive power scores we use pandas categorical data type for class_cat\n",
    "    class_categories_pd = CategoricalDtype(categories=target_classes_int, ordered=True)\n",
    "    X_original_y_cat_pd = X_original_y_pd.copy()\n",
    "    X_original_y_cat_pd[target_column_cat_name] = X_original_y_pd[target_column_name].astype(class_categories_pd)\n",
    "    X_original_y_cat_pd.drop(target_column_name, axis=1, inplace=True)\n",
    "\n",
    "    #predictive_power_score_matrix_all_pd = pps.matrix(df_pd_all, output='df')\n",
    "    predictive_power_scores_pd = pps.predictors(X_original_y_cat_pd, y=target_column_cat_name, output='df')\n",
    "    predictive_power_scores_pd.style.background_gradient(cmap='twilight', low=0.0, high=1.0)\n",
    "    print(predictive_power_scores_pd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define a function to train a Decision Tree Classifier\n",
    "\n",
    "def train_decision_tree_classifier(X_train_np, y_train_np, max_depth):\n",
    "    dtc = tree.DecisionTreeClassifier(random_state=42, max_depth=max_depth)\n",
    "    dtc.fit(X_train_np, y_train_np)\n",
    "    return dtc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define a function to train a Support Vector Machine Classifier\n",
    "\n",
    "def train_support_vector_machine_classifier(X_train_np, y_train_np, C):\n",
    "    svmc = svm.SVC(C=C, kernel='linear', probability=True)\n",
    "    svmc.fit(X_train_np, y_train_np)\n",
    "    return svmc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we define a function to assert all elements in an array are 0 or 1\n",
    "\n",
    "def assert_elements_are_zero_or_one(a):\n",
    "    assert len(np.ma.masked_where(((a == 0) | (a == 1)).all(), a).compressed()) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code block we compute a binary confusion matrix with corresponding metrics\n",
    "\n",
    "def compute_binary_confusion_matrix(y_test_np, y_test_pred_class_np):\n",
    "    assert y_test_np.shape == y_test_pred_class_np.shape\n",
    "    assert y_test_np.shape == (y_test_np.shape[0],)\n",
    "    assert_elements_are_zero_or_one(y_test_np)\n",
    "    assert_elements_are_zero_or_one(y_test_pred_class_np)\n",
    "    \n",
    "    n_samples = y_test_np.shape[0]\n",
    "    assert n_samples == y_test_pred_class_np.shape[0]\n",
    "\n",
    "    n_p = y_test_np.sum()\n",
    "    n_n = ((y_test_np - 1) * -1).sum()\n",
    "    assert n_p + n_n == n_samples\n",
    "\n",
    "    y_tmp = (y_test_np * 2) - y_test_pred_class_np\n",
    "    n_tp = np.count_nonzero(y_tmp == 1) # positive successfully classified as positive\n",
    "    n_fn = np.count_nonzero(y_tmp == 2) # positive incorrectly classified as negative\n",
    "    n_tn = np.count_nonzero(y_tmp == 0) # negative successfully classified as negative\n",
    "    n_fp = np.count_nonzero(y_tmp == -1) # negative incorrectly classified as positive\n",
    "    assert n_tp + n_fp + n_tn + n_fn == n_samples\n",
    "\n",
    "    cm = np.asarray([[n_tp, n_fn], [n_fp, n_tn]])\n",
    "\n",
    "    #print(f\"n_samples={n_samples}\")\n",
    "    #print(f\"n_p={n_p}\")\n",
    "    #print(f\"n_n={n_n}\")\n",
    "    #print(f\"n_tp={n_tp}\")\n",
    "    #print(f\"n_fp={n_fp}\")\n",
    "    #print(f\"n_tn={n_tn}\")\n",
    "    #print(f\"n_fn={n_fn}\")\n",
    "    #print(f\"confusion_matrix=\\n{cm}\")\n",
    "\n",
    "    return n_samples, n_p, n_n, cm\n",
    "    \n",
    "def compute_tpr___recall___sensitivity(n_tp, n_p):\n",
    "    # True Positive Rate\n",
    "    # TPR = TP / P = 1 - FNR\n",
    "    # Intuition: What percentage of the positive samples are correctly classified as positive?\n",
    "    # Intuition: 100% of positive samples minus the percentage of positive samples incorrectly classified.\n",
    "    return n_tp / n_p\n",
    "\n",
    "def compute_fnr___miss_rate(n_fn, n_p):\n",
    "    # False Negative Rate\n",
    "    # FNR = FN / P = 1 - TPR\n",
    "    # Intuition: What percentage of the positive samples are incorrectly classified as negative?\n",
    "    # Intuition: 100% of positive samples minus the percentage of positive samples correctly classified.\n",
    "    return n_fn / n_p\n",
    "\n",
    "def compute_fpr___fall_out___false_alarm(n_fp, n_n):\n",
    "    # False Positive Rate\n",
    "    # FPR = FP / N = 1 - TNR\n",
    "    # Intuition: What percentage of the negative samples are incorrectly classified as positive?\n",
    "    # Intuition: 100% of negative samples minus the percentage of negative samples correctly classified.\n",
    "    return n_fp / n_n\n",
    "\n",
    "def compute_tnr___specificity(n_tn, n_n):\n",
    "    # True Negative Rate\n",
    "    # TNR = TN / N = 1 - FPR\n",
    "    # Intuition: What percentage of the negative samples are correctly classified as negative?\n",
    "    # Intuition: 100% of negative samples minus the percentage of negative samples incorrectly classified.\n",
    "    return n_tn / n_n\n",
    "\n",
    "def compute_tpr_fnr_fpr_tnr(n_p, n_n, n_tp, n_fn, n_fp, n_tn):\n",
    "    tpr = compute_tpr___recall___sensitivity(n_tp, n_p)\n",
    "    fnr = compute_fnr___miss_rate(n_fn, n_p)\n",
    "    fpr = compute_fpr___fall_out___false_alarm(n_fp, n_n)\n",
    "    tnr = compute_tnr___specificity(n_tn, n_n)\n",
    "    #print(f\"tpr={tpr:.2f}\")\n",
    "    #print(f\"fnr={fnr:.2f}\")\n",
    "    #print(f\"fpr={fpr:.2f}\")\n",
    "    #print(f\"tnr={tnr:.2f}\")\n",
    "    return tpr, fnr, fpr, tnr\n",
    "\n",
    "def compute_informedness(tpr, tnr):\n",
    "    # Informedness aka. Youden's J statistic\n",
    "    # J = TPR + TNR - 1\n",
    "    # Ranges from -1 through 1 (inclusive).\n",
    "    # Usage: A way of summarizing the performance of a diagnostic test.\n",
    "    # Intuition: 1 means the test is perfect.\n",
    "    # Intuition: 0 means the test is useless like throwing a coin.\n",
    "    # Intuition: Negative values indicate the test is wrong and might have bugs.\n",
    "    informedness = tpr + tnr - 1\n",
    "    #print(f\"informedness={informedness}\")\n",
    "    return informedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_iris_dataset_made_binary():\n",
    "    n_targets = 1\n",
    "    n_target_classes = 3\n",
    "\n",
    "    target_column_name = 'species'\n",
    "\n",
    "    columns_to_drop = ['Id']\n",
    "\n",
    "    column_name_mapping = {'SepalLengthCm': 'sepal_length',\n",
    "        'SepalWidthCm': 'sepal_width',\n",
    "        'PetalLengthCm': 'petal_length',\n",
    "        'PetalWidthCm': 'petal_width',\n",
    "        'Species': 'species'}\n",
    "\n",
    "    # we make the dataset binary by mapping two classes to 0 just for experimentation!\n",
    "    target_class_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 0, 'Iris-virginica': 1}\n",
    "    assert len(target_class_mapping) == n_target_classes   \n",
    "\n",
    "    X_original_pd, y_pd, X_original_np, y_np, X_original, y, X_original_y_pd, X_original_y = load_dataset(\"../../data/iris.csv\",\n",
    "        ',', target_column_name, columns_to_drop, column_name_mapping, target_class_mapping)\n",
    "\n",
    "    print(X_original_y_pd)\n",
    "\n",
    "    X_np, X = standardize_dataset(X_original_np)\n",
    "\n",
    "    X_train, y_train, X_train_np, y_train_np, X_val, X_val_np, y_val, y_val_np, X_test, X_test_np, y_test, y_test_np = split_dataset(X, y, n_targets)\n",
    "\n",
    "    #explore_dataset(X_original_y_pd, X_original_y_pd.to_numpy(), n_target_classes, 'species')\n",
    "\n",
    "    #perform_pca(3, X_original_np, X_original_y_pd, target_column_name)\n",
    "\n",
    "    return X_train, y_train, X_train_np, y_train_np, X_val, X_val_np, y_val, y_val_np, X_test, X_test_np, y_test, y_test_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roc_graph(y_test_np, y_test_pred_proba_p_np):\n",
    "    thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "    print(np.round(y_test_pred_proba_p_np, 2))\n",
    "    l = []\n",
    "    for threshold in thresholds:\n",
    "        y_test_pred = y_test_pred_proba_p_np.copy()\n",
    "        y_test_pred[y_test_pred >= threshold] = 1\n",
    "        y_test_pred[y_test_pred < threshold] = 0\n",
    "        #print(y_test_pred)\n",
    "        n_samples, n_p, n_n, cm = compute_binary_confusion_matrix(y_test_np, y_test_pred)\n",
    "        tpr, fnr, fpr, tnr = compute_tpr_fnr_fpr_tnr(n_p, n_n, cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1])\n",
    "        l.append([threshold, cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1], tpr, fnr, fpr, tnr])\n",
    "    a = np.asarray(l)\n",
    "    return a\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_iris_dataset_made_binary():\n",
    "    X_train, y_train, X_train_np, y_train_np, X_val, X_val_np, y_val, y_val_np, X_test, X_test_np, y_test, y_test_np = load_and_preprocess_iris_dataset_made_binary()\n",
    "\n",
    "    #clf = train_decision_tree_classifier(X_train_np, y_train_np, 1)\n",
    "\n",
    "    # for this example we use small regularization parameter C to get more uncertain classifications\n",
    "    svm_regularization_parameter_C = 0.0007\n",
    "    clf = train_support_vector_machine_classifier(X_train_np, y_train_np, svm_regularization_parameter_C)\n",
    "\n",
    "    y_test_pred_class_np = clf.predict(X_test)\n",
    "    y_test_pred_proba_np = clf.predict_proba(X_test).transpose()\n",
    "    # y_test_pred_proba_np[0] is the predicted probability that the sample is negative i.e. class 0\n",
    "    y_test_pred_proba_n_np = y_test_pred_proba_np[0]\n",
    "    # y_test_pred_proba_np[1] is the predicted probability that the sample is positive i.e. class 1\n",
    "    y_test_pred_proba_p_np = y_test_pred_proba_np[1]\n",
    "\n",
    "    y_test_pred_np = np.row_stack((y_test_np, y_test_pred_class_np, y_test_pred_proba_n_np, y_test_pred_proba_p_np, y_test_pred_proba_n_np + y_test_pred_proba_p_np))\n",
    "    print(np.round(y_test_pred_np.transpose(), 2))\n",
    "\n",
    "    n_samples, n_p, n_n, cm = compute_binary_confusion_matrix(y_test_np, y_test_pred_class_np)\n",
    "\n",
    "    cm_sklearn = confusion_matrix(y_test_np, y_test_pred_class_np, labels=[1, 0])\n",
    "    print(f\"cm_sklearn=\\n{cm_sklearn}\")\n",
    "    print(f\"tp fn fp tn={cm_sklearn.ravel()}\")\n",
    "    print(f\"type(cm_sklearn)={type(cm_sklearn)}\")\n",
    "    assert (cm == cm_sklearn).all()\n",
    "\n",
    "    tpr, fnr, fpr, tnr = compute_tpr_fnr_fpr_tnr(n_p, n_n, cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1])\n",
    "\n",
    "    informedness = compute_informedness(tpr, tnr)\n",
    "\n",
    "    a = create_roc_graph(y_test_np, y_test_pred_proba_p_np)\n",
    "\n",
    "    print(np.round(a, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples=150\n",
      "n_targets=1\n",
      "n_features=4\n",
      "     sepal_length  sepal_width  petal_length  petal_width  species\n",
      "0             5.1          3.5           1.4          0.2        0\n",
      "1             4.9          3.0           1.4          0.2        0\n",
      "2             4.7          3.2           1.3          0.2        0\n",
      "3             4.6          3.1           1.5          0.2        0\n",
      "4             5.0          3.6           1.4          0.2        0\n",
      "..            ...          ...           ...          ...      ...\n",
      "145           6.7          3.0           5.2          2.3        1\n",
      "146           6.3          2.5           5.0          1.9        1\n",
      "147           6.5          3.0           5.2          2.0        1\n",
      "148           6.2          3.4           5.4          2.3        1\n",
      "149           5.9          3.0           5.1          1.8        1\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "[[0.   0.   0.89 0.11 1.  ]\n",
      " [1.   0.   0.52 0.48 1.  ]\n",
      " [1.   0.   0.62 0.38 1.  ]\n",
      " [0.   0.   0.99 0.01 1.  ]\n",
      " [0.   0.   0.98 0.02 1.  ]\n",
      " [1.   0.   0.31 0.69 1.  ]\n",
      " [1.   0.   0.2  0.8  1.  ]\n",
      " [1.   0.   0.79 0.21 1.  ]\n",
      " [0.   0.   0.98 0.02 1.  ]\n",
      " [0.   0.   0.89 0.11 1.  ]\n",
      " [0.   0.   1.   0.   1.  ]\n",
      " [1.   0.   0.61 0.39 1.  ]\n",
      " [0.   0.   0.99 0.01 1.  ]\n",
      " [0.   0.   0.98 0.02 1.  ]\n",
      " [0.   0.   1.   0.   1.  ]\n",
      " [1.   0.   0.06 0.94 1.  ]\n",
      " [1.   0.   0.02 0.98 1.  ]\n",
      " [1.   0.   0.06 0.94 1.  ]\n",
      " [0.   0.   1.   0.   1.  ]\n",
      " [0.   0.   0.91 0.09 1.  ]\n",
      " [0.   0.   1.   0.   1.  ]\n",
      " [1.   0.   0.11 0.89 1.  ]\n",
      " [1.   0.   0.17 0.83 1.  ]\n",
      " [0.   0.   0.65 0.35 1.  ]\n",
      " [0.   0.   0.61 0.39 1.  ]\n",
      " [0.   0.   0.96 0.04 1.  ]\n",
      " [1.   0.   0.29 0.71 1.  ]\n",
      " [0.   0.   1.   0.   1.  ]\n",
      " [0.   0.   0.26 0.74 1.  ]\n",
      " [1.   0.   0.92 0.08 1.  ]]\n",
      "cm_sklearn=\n",
      "[[ 0 13]\n",
      " [ 0 17]]\n",
      "tp fn fp tn=[ 0 13  0 17]\n",
      "type(cm_sklearn)=<class 'numpy.ndarray'>\n",
      "[0.11 0.48 0.38 0.01 0.02 0.69 0.8  0.21 0.02 0.11 0.   0.39 0.01 0.02\n",
      " 0.   0.94 0.98 0.94 0.   0.09 0.   0.89 0.83 0.35 0.39 0.04 0.71 0.\n",
      " 0.74 0.08]\n",
      "[[ 0.  13.   0.  17.   0.   1.   0.   1.   0. ]\n",
      " [ 0.  13.   0.  11.   6.   1.   0.   0.6  0.4]\n",
      " [ 0.  13.   0.  10.   7.   1.   0.   0.6  0.4]\n",
      " [ 0.  13.   0.   7.  10.   1.   0.   0.4  0.6]\n",
      " [ 0.  13.   0.   7.  10.   1.   0.   0.4  0.6]\n",
      " [ 0.  13.   0.   6.  11.   1.   0.   0.4  0.6]\n",
      " [ 0.1 13.   0.   6.  11.   1.   0.   0.4  0.6]\n",
      " [ 0.1 13.   0.   6.  11.   1.   0.   0.4  0.6]\n",
      " [ 0.1 13.   0.   6.  11.   1.   0.   0.4  0.6]\n",
      " [ 0.1 12.   1.   5.  12.   0.9  0.1  0.3  0.7]\n",
      " [ 0.1 12.   1.   5.  12.   0.9  0.1  0.3  0.7]\n",
      " [ 0.1 12.   1.   4.  13.   0.9  0.1  0.2  0.8]\n",
      " [ 0.1 12.   1.   3.  14.   0.9  0.1  0.2  0.8]\n",
      " [ 0.1 12.   1.   3.  14.   0.9  0.1  0.2  0.8]\n",
      " [ 0.1 12.   1.   3.  14.   0.9  0.1  0.2  0.8]\n",
      " [ 0.2 12.   1.   3.  14.   0.9  0.1  0.2  0.8]\n",
      " [ 0.2 12.   1.   3.  14.   0.9  0.1  0.2  0.8]\n",
      " [ 0.2 12.   1.   3.  14.   0.9  0.1  0.2  0.8]\n",
      " [ 0.2 12.   1.   3.  14.   0.9  0.1  0.2  0.8]\n",
      " [ 0.2 12.   1.   3.  14.   0.9  0.1  0.2  0.8]\n",
      " [ 0.2 12.   1.   3.  14.   0.9  0.1  0.2  0.8]\n",
      " [ 0.2 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.2 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.2 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.2 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.2 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.3 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.3 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.3 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.3 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.3 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.3 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.3 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.3 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.3 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.4 11.   2.   3.  14.   0.8  0.2  0.2  0.8]\n",
      " [ 0.4 11.   2.   2.  15.   0.8  0.2  0.1  0.9]\n",
      " [ 0.4 11.   2.   2.  15.   0.8  0.2  0.1  0.9]\n",
      " [ 0.4 10.   3.   2.  15.   0.8  0.2  0.1  0.9]\n",
      " [ 0.4  9.   4.   2.  15.   0.7  0.3  0.1  0.9]\n",
      " [ 0.4  9.   4.   1.  16.   0.7  0.3  0.1  0.9]\n",
      " [ 0.4  9.   4.   1.  16.   0.7  0.3  0.1  0.9]\n",
      " [ 0.4  9.   4.   1.  16.   0.7  0.3  0.1  0.9]\n",
      " [ 0.4  9.   4.   1.  16.   0.7  0.3  0.1  0.9]\n",
      " [ 0.4  9.   4.   1.  16.   0.7  0.3  0.1  0.9]\n",
      " [ 0.4  9.   4.   1.  16.   0.7  0.3  0.1  0.9]\n",
      " [ 0.5  9.   4.   1.  16.   0.7  0.3  0.1  0.9]\n",
      " [ 0.5  9.   4.   1.  16.   0.7  0.3  0.1  0.9]\n",
      " [ 0.5  9.   4.   1.  16.   0.7  0.3  0.1  0.9]\n",
      " [ 0.5  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.5  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.5  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.5  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.5  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.5  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.6  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.7  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.7  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.7  8.   5.   1.  16.   0.6  0.4  0.1  0.9]\n",
      " [ 0.7  7.   6.   1.  16.   0.5  0.5  0.1  0.9]\n",
      " [ 0.7  7.   6.   1.  16.   0.5  0.5  0.1  0.9]\n",
      " [ 0.7  6.   7.   1.  16.   0.5  0.5  0.1  0.9]\n",
      " [ 0.7  6.   7.   1.  16.   0.5  0.5  0.1  0.9]\n",
      " [ 0.7  6.   7.   1.  16.   0.5  0.5  0.1  0.9]\n",
      " [ 0.7  6.   7.   0.  17.   0.5  0.5  0.   1. ]\n",
      " [ 0.8  6.   7.   0.  17.   0.5  0.5  0.   1. ]\n",
      " [ 0.8  6.   7.   0.  17.   0.5  0.5  0.   1. ]\n",
      " [ 0.8  6.   7.   0.  17.   0.5  0.5  0.   1. ]\n",
      " [ 0.8  6.   7.   0.  17.   0.5  0.5  0.   1. ]\n",
      " [ 0.8  6.   7.   0.  17.   0.5  0.5  0.   1. ]\n",
      " [ 0.8  6.   7.   0.  17.   0.5  0.5  0.   1. ]\n",
      " [ 0.8  5.   8.   0.  17.   0.4  0.6  0.   1. ]\n",
      " [ 0.8  5.   8.   0.  17.   0.4  0.6  0.   1. ]\n",
      " [ 0.8  4.   9.   0.  17.   0.3  0.7  0.   1. ]\n",
      " [ 0.8  4.   9.   0.  17.   0.3  0.7  0.   1. ]\n",
      " [ 0.8  4.   9.   0.  17.   0.3  0.7  0.   1. ]\n",
      " [ 0.9  4.   9.   0.  17.   0.3  0.7  0.   1. ]\n",
      " [ 0.9  4.   9.   0.  17.   0.3  0.7  0.   1. ]\n",
      " [ 0.9  4.   9.   0.  17.   0.3  0.7  0.   1. ]\n",
      " [ 0.9  4.   9.   0.  17.   0.3  0.7  0.   1. ]\n",
      " [ 0.9  3.  10.   0.  17.   0.2  0.8  0.   1. ]\n",
      " [ 0.9  3.  10.   0.  17.   0.2  0.8  0.   1. ]\n",
      " [ 0.9  3.  10.   0.  17.   0.2  0.8  0.   1. ]\n",
      " [ 0.9  3.  10.   0.  17.   0.2  0.8  0.   1. ]\n",
      " [ 0.9  2.  11.   0.  17.   0.2  0.8  0.   1. ]\n",
      " [ 1.   1.  12.   0.  17.   0.1  0.9  0.   1. ]\n",
      " [ 1.   1.  12.   0.  17.   0.1  0.9  0.   1. ]\n",
      " [ 1.   1.  12.   0.  17.   0.1  0.9  0.   1. ]\n",
      " [ 1.   0.  13.   0.  17.   0.   1.   0.   1. ]\n",
      " [ 1.   0.  13.   0.  17.   0.   1.   0.   1. ]\n",
      " [ 1.   0.  13.   0.  17.   0.   1.   0.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "play_with_iris_dataset_made_binary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad23019639f4f0cec1969a685ba2a43be98d322ffbcf7e3409a6239a86c6a8ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
